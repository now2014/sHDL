[{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"inroduction","dir":"Articles","previous_headings":"","what":"Inroduction","title":"Genomic annotations","text":"marks significant breakthrough sHDL integrates interpretation framework binary continuous genomic annotations. tutorial illustrate process estimating heritability enrichment fold following acquisition genomic features. !!!Caution!!!: weights different annotations aren’t comparable, direct cross-annotation comparisons enrichment folds avoided. cases, comparing p-values enrichment fold estimation appropriate.","code":""},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Genomic annotations","text":"Suppose gone tutorial prepared LD reference panel: UKB_array_SVD_eigen90_extraction, downloaded gene locations information GeneBreak package: ens.gene.ann.hg19.rda, installed sHDL HDL packages. Additionally, download gene specificity data brain cell types (Skene et al. 2017) hjerling leffler lab. Data contents tutorial: use gene specificity ASC cell type (Astrocytes) annotation ctd_DRONC_human.rda file illustration.","code":"wget -c http://www.hjerling-leffler-lab.org/data/scz_singlecell/ctdFiles.zip  unzip ctdFiles.zip tree ./                                 # ./ # ├── ctd_AIBS.rda # ├── ctd_allKI.rda # ├── ctd_DRONC_human.rda # ├── ctd_DRONC_mouse.rda # ├── ens.gene.ann.hg19.rda  # gene locations # └── UKB_array_SVD_eigen90_extraction  # LD reference panel load('ctd_DRONC_human.rda') print(ctd[[1]]$specificity[1:5, 1:5]) #                  ASC       END        exCA        exDG       exPFC # A1BG     0.065780027 0.0000000 0.134620574 0.108029999 0.073845381 # A1BG-AS1 0.021622098 0.1625062 0.168150754 0.113631342 0.197219791 # A1CF     0.064952275 0.2440825 0.053170623 0.000000000 0.000000000 # A2M      0.003421627 0.8803978 0.006920069 0.005024324 0.004067106 # A2M-AS1  0.035128297 0.3960231 0.028756397 0.069229097 0.083800251  gene.weights <- ctd[[1]]$specificity[, 'ASC']  summary(gene.weights) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  # 0.00000 0.02942 0.05097 0.06333 0.07369 1.00000"},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"binary-annotation","dir":"Articles","previous_headings":"","what":"Binary annotation","title":"Genomic annotations","text":"Step 01. create vector D containing annotated SNPs located 10% genes highest specificity scores. Step 02. Run sHDL binary annotation birth weight GWAS summary statistics HDL.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction'  ## extract the top 10% genes ## q90 <- quantile(gene.weights, 0.9) top10.genes <- names(gene.weights)[gene.weights > q90]  ## match gene locations ## load('ens.gene.ann.hg19.rda') # load ens.gene.ann.hg19 idx <- ens.gene.ann.hg19$Gene %in% top10.genes gene.pos <- unique(ens.gene.ann.hg19[idx, c(3:5)]) colnames(gene.pos) <- c('chrom', 'start', 'end') autosomes <- as.character(1:22) gene.pos <- gene.pos[gene.pos$chrom %in% autosomes, ] # keep autosomes only gene.pos$chrom <- as.numeric(gene.pos$chrom)  ## extract variants position from LD reference panel ## bim <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE )) bim <- bim[, c(2, 1, 4)] colnames(bim) <- c('SNP', 'chrom', 'pos')  ## match annotated variants ## gene2snp <- function(gene.pos, bim, mc.cores=4){   map.gene <- function(i, gene.pos.chr, bim.chr){     s <- gene.pos.chr[i, 'start']     e <- gene.pos.chr[i, 'end']     return(bim.chr[bim.chr$pos >= s & bim.chr$pos <= e, ]$SNP)   }   chroms <- unique(bim$chrom)   all.snps <- c()   for(cur.chrom in chroms){     gene.pos.chr <- gene.pos[gene.pos$chrom == cur.chrom, ]     bim.chr <- bim[bim$chrom == cur.chrom, ]     snps <- parallel::mclapply(       seq_len(nrow(gene.pos.chr)), map.gene,       gene.pos.chr=gene.pos.chr, bim.chr=bim.chr,       mc.cores=mc.cores     )     snps <- unique(unlist(snps))     all.snps <- c(all.snps, snps)   }   return(all.snps) } snps <- gene2snp(gene.pos, bim, 4)  ## create the annotation weights for sHDL ## D <- matrix(1, nrow=length(snps), ncol=1, dimnames=list(snps, 'Top10.genes')) data(gwas1.example, package='HDL') res <- sHDL::sHDL(   D, gwas1.example, LD.path, mc.cores=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL )  # print(knitr::kable(res))  # |item             | estimation|        se|         p|note                                             | # |:----------------|----------:|---------:|---------:|:------------------------------------------------| # |time             | 60.7460063|        NA|        NA|seconds                                          | # |h2               |  0.1639165| 0.0054560| 0.0000000|total heritability                               | # |intercept        |  0.9632359| 0.0106590| 0.0000000|intercept                                        | # |fold.Top10.genes |  1.3037835| 0.1047777| 0.0037398|enrichment fold                                  | # |converged        |         NA|        NA|        NA|TRUE                                             | # |message          |         NA|        NA|        NA|CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL |"},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"continuous-annotation","dir":"Articles","previous_headings":"","what":"Continuous annotation","title":"Genomic annotations","text":"context continuous annotation, normalization process employed ensure annotation fits sHDL model. Step 01. Extract gene locations (hg19) weights. Step 02. Map gene weights SNP weights. Step 03. Check weights variants.","code":"## extract gene infomation ## load('ens.gene.ann.hg19.rda') # load ens.gene.ann.hg19 gene.df <- unique(ens.gene.ann.hg19[, c(1, 3:5)]) colnames(gene.df) <- c('gene', 'chrom', 'start', 'end') autosomes <- as.character(1:22) gene.df <- gene.df[gene.df$chrom %in% autosomes, ] # keep autosomes only gene.df$chrom <- as.numeric(gene.df$chrom) gene.df <- gene.df[!duplicated(gene.df$gene), ] # remove duplicated genes gene.df$weight <- gene.weights[gene.df$gene] gene.df <- gene.df[!is.na(gene.df$weight), ] # remove genes without weight  head(gene.df, 5) #            gene chrom  start    end     weight # 27828 LINC00115     1 761586 762902 0.13639639 # 12129    SAMD11     1 860260 879955 0.52717077 # 12571     NOC2L     1 879584 894689 0.05715007 # 12699    KLHL17     1 895967 901095 0.07142173 # 13025      HES4     1 934342 935552 0.09547616 LD.path <- 'UKB_array_SVD_eigen90_extraction'  ## extract variants position from LD reference panel ## bim <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE )) bim <- bim[, c(2, 1, 4)] colnames(bim) <- c('SNP', 'chrom', 'pos')  ## map weights ## mapw <- function(i, j){   cur.chrom <- gene.df[j, 'chrom']   s <- gene.df[j, 'start']   e <- gene.df[j, 'end']   w <- gene.df[j, 'weight']   bim <- bim[bim$chrom == cur.chrom, ]   bim <- bim[bim$pos >= s & bim$pos <= e, ]    snps <- unique(c(names(i), bim$SNP))   nsnps <- length(snps)   snps.w <- rep(0, nsnps)   names(snps.w) <- snps   snps.w[names(i)] <- snps.w[names(i)] + i   snps.w[bim$SNP] <- snps.w[bim$SNP] + w   if(j %% 1000 == 0) cat(sprintf(     '# of processed genes %d: # of annotated %d variants (%.2f%%) \\n',     j, nsnps, nsnps/M*100   ))   return(snps.w) } M <- nrow(bim) snps.w <- Reduce(mapw, 1:nrow(gene.df), init=c()) D <- matrix(snps.w, ncol=1, dimnames = list(names(snps.w), 'raw.W')) saveRDS(D, 'raw.W.rds') summary(D)  #     raw.W          # Min.   :0.00000    # 1st Qu.:0.02038    # Median :0.04425    # Mean   :0.06551    # 3rd Qu.:0.07747    # Max.   :1.65767"},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"min-max-normalization","dir":"Articles","previous_headings":"Continuous annotation","what":"Min-max normalization","title":"Genomic annotations","text":"One straightforward normalization method involves min-max normalization, normalized D value signifies degree selected variant pertains annotated category. Run sHDL minmax normalization.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction' D <- readRDS('raw.W.rds') colnames(D) <- 'raw.W.minmax' D.minmax <- sHDL:::normD(D, LD.path, norm.method='minmax')  # Annotation name(s): [raw.W.minmax]. # Applied `minmax` weight nomalization on [121235 (39.424%)] annotated variants. The theoretical upper boundary for enrichment fold is M / Md = [61.419].  summary(D.minmax)  #  raw.W.minmax      # Min.   :0.00000  # 1st Qu.:0.00000  # Median :0.00000  # Mean   :0.01628  # 3rd Qu.:0.02030  # Max.   :1.00000 LD.path <- 'UKB_array_SVD_eigen90_extraction' data(gwas1.example, package='HDL') D <- readRDS('raw.W.rds') colnames(D) <- 'raw.W.minmax' res <- sHDL::sHDL(   D, gwas1.example, LD.path, mc.cores=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL, norm.method='minmax' )  # print(str(res)) # 'data.frame':   6 obs. of  5 variables: #  $ item      : chr  \"time\" \"h2\" \"intercept\" \"fold.raw.W.minmax\" ... #  $ estimation: num  56.354 0.164 0.963 8.741 NA ... #  $ se        : num  NA 0.00546 0.01066 0.90955 NA ... #  $ p         : num  NA 2.69e-198 0.00 1.72e-17 NA ... #  $ note      : chr  \"seconds\" \"total heritability\" \"intercept\" \"enrichment fold\" ..."},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"scaled-normalization","dir":"Articles","previous_headings":"Continuous annotation","what":"Scaled normalization","title":"Genomic annotations","text":"Alternatively, another approach scale sum(D) number annotated variants. Run sHDL scaled normalization.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction' D <- readRDS('raw.W.rds') colnames(D) <- 'raw.W.scaled' D.scaled <- sHDL:::normD(D, LD.path, norm.method='scaled')  # Annotation name(s): [raw.W.scaled]. # Applied `scaled` weight nomalization on [121235 (39.424%)] annotated variants. The theoretical upper boundary for enrichment fold is M / Md = [2.537].  summary(D.scaled)  #  raw.W.scaled      # Min.   : 0.0000    # 1st Qu.: 0.0000    # Median : 0.0000    # Mean   : 0.3942    # 3rd Qu.: 0.4916    # Max.   :24.2134 LD.path <- 'UKB_array_SVD_eigen90_extraction' data(gwas1.example, package='HDL') D <- readRDS('raw.W.rds') colnames(D) <- 'raw.W.scaled' res <- sHDL::sHDL(   D, gwas1.example, LD.path, mc.cores=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL, norm.method='scaled' )  # print(str(res)) # 'data.frame':   6 obs. of  5 variables: #  $ item      : chr  \"time\" \"h2\" \"intercept\" \"fold.raw.W.scaled\" ... #  $ estimation: num  64.204 0.164 0.963 1.197 NA ... #  $ se        : num  NA 0.00546 0.01066 0.02313 NA ... #  $ p         : num  NA 2.69e-198 0.00 1.73e-17 NA ... #  $ note      : chr  \"seconds\" \"total heritability\" \"intercept\" \"enrichment fold\" ..."},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"no-normalization","dir":"Articles","previous_headings":"Continuous annotation","what":"No normalization","title":"Genomic annotations","text":"One can also choose keep original annotation weights without normalization. Run sHDL without annotation weights normalization.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction' D <- readRDS('raw.W.rds') D.none <- sHDL:::normD(D, LD.path, norm.method='none')  # Annotation name(s): [raw.W]. # No normalization applied on [121235 (39.424%)] annotated variants. The theoretical upper boundary for enrichment fold is M / Md = [37.051].  summary(D.none)  #     raw.W          # Min.   :0.00000    # 1st Qu.:0.00000    # Median :0.00000    # Mean   :0.02699    # 3rd Qu.:0.03366    # Max.   :1.65767 LD.path <- 'UKB_array_SVD_eigen90_extraction' data(gwas1.example, package='HDL') D <- readRDS('raw.W.rds') res <- sHDL::sHDL(   D, gwas1.example, LD.path, mc.cores=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL, norm.method='none' )  # print(str(res)) # 'data.frame':   6 obs. of  5 variables: #  $ item      : chr  \"time\" \"h2\" \"intercept\" \"fold.raw.W\" ... #  $ estimation: num  63.831 0.164 0.963 5.619 NA ... #  $ se        : num  NA 0.00546 0.01066 0.54271 NA ... #  $ p         : num  NA 2.69e-198 0.00 1.73e-17 NA ... #  $ note      : chr  \"seconds\" \"total heritability\" \"intercept\" \"enrichment fold\" ..."},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Example of sHDL","text":"install latest version sHDL package via Github, run following code R: data.table package also required example:","code":"# install.packages('remotes') remotes::install_github(\"now2014/sHDL\", ref='main') install.packages('data.table')"},{"path":[]},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"pre-computed-ld-reference-panels","dir":"Articles","previous_headings":"Data preparation","what":"Pre-computed LD reference panels","title":"Example of sHDL","text":"three publically available pre-computed autosome LD reference panels European-ancestry population (335,265 Genomic British individuals UK Biobank) HDL: Panel 1. QCed UK Biobank Axiom Array (307,519 SNPs): Panel 2. QCed UK Biobank imputed HapMap2 (769,306 SNPs): Panel 3. QCed UK Biobank imputed HapMap3 (1,029,876 SNPs): may download one reference panel according SNP density GWAS summary statistics, build LD reference panel. Alternatively, can download reference panels via Baidu Netdisk referring FAQ. , illustrate usage sHDL Panel 2. QCed UK Biobank Axiom Array. Step 01. Download LD reference panel. Step 02. Extract files archive.","code":"wget -c -t 1 \\   https://www.dropbox.com/s/fuvpwsf6r8tjd6c/UKB_array_SVD_eigen90_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_array_SVD_eigen90_extraction.tar.gz wget -c -t 1 \\   https://www.dropbox.com/s/4vuktycxz1an6sp/UKB_imputed_hapmap2_SVD_eigen99_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_imputed_hapmap2_SVD_eigen99_extraction.tar.gz wget -c -t 1 \\   https://www.dropbox.com/s/6js1dzy4tkc3gac/UKB_imputed_SVD_eigen99_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_imputed_SVD_eigen99_extraction.tar.gz wget -c -t 1 https://www.dropbox.com/s/fuvpwsf6r8tjd6c/UKB_array_SVD_eigen90_extraction.tar.gz?dl=0 \\ --no-check-certificate -O /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz # checking the md5 = ff3fadd7ea08bd29759b6c652618cd1f md5sum /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz  # extraction tar -xvf /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"gwas-summary-statistics","dir":"Articles","previous_headings":"Data preparation","what":"GWAS summary statistics","title":"Example of sHDL","text":"can test dataset HDL directly: can prepare data follows: Step 01. Download Height GWAS summary statistics GIANT consortium. Step 02. Load GWAS summary statistics R. Step 03. Select & rename 4 required columns sHDL: SNP: rsid variants A1: effect allele A2: allele Z: GWAS Z-score N: Sample size","code":"data(gwas1.example, package='HDL') # GWAS summary statistics from the Neale Lab round 2 GWAS of UK Biobank of birth weight. gwas.df <- gwas1.example[, c('SNP', 'A1', 'A2', 'Z', 'N')] saveRDS(gwas1.example, file='gwas.df.rds') # save the gwas.df for sHDL wget -c https://portals.broadinstitute.org/collaboration/giant/images/f/f7/GIANT_HEIGHT_YENGO_2022_GWAS_SUMMARY_STATS_EUR.gz gwas.df <- data.table::fread('/path/to/GIANT_HEIGHT_YENGO_2022_GWAS_SUMMARY_STATS_EUR.gz', fill=TRUE, header=TRUE)  head(gwas.df, 5) # show the first five rows #           SNPID       RSID CHR    POS EFFECT_ALLELE OTHER_ALLELE EFFECT_ALLELE_FREQ         BETA      SE        P       N # 1: 1:566875:C:T  rs2185539   1 566875             T            C           0.002800 -0.046315600 0.03930 0.238128  537968 # 2: 1:728951:C:T rs11240767   1 728951             T            C           0.000356  0.167358000 0.12600 0.185025   85591 # 3: 1:734462:A:G rs12564807   1 734462             A            G           0.893000  0.004656900 0.01100 0.672866  112953 # 4: 1:752721:A:G  rs3131972   1 752721             G            A           0.840000  0.000544089 0.00284  0.84811  615932 # 5: 1:754182:A:G  rs3131969   1 754182             G            A           0.865000  0.001333110 0.00185 0.470389 1100634 gwas.df$Z <- gwas.df$BETA / gwas.df$SE # calculate the GWAS Z-score gwas.df <- gwas.df[, c('RSID', 'EFFECT_ALLELE', 'OTHER_ALLELE', 'Z', 'N')] colnames(gwas.df) <- c('SNP', 'A1', 'A2', 'Z', 'N')  head(gwas.df, 5) #           SNP A1 A2          Z       N # 1:  rs2185539  T  C -1.1785140  537968 # 2: rs11240767  T  C  1.3282381   85591 # 3: rs12564807  A  G  0.4233545  112953 # 4:  rs3131972  G  A  0.1915806  615932 # 5:  rs3131969  G  A  0.7206000 1100634  saveRDS(gwas.df, file='gwas.df.rds') # save the gwas.df for sHDL"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"genomic-annotation","dir":"Articles","previous_headings":"Data preparation","what":"Genomic annotation","title":"Example of sHDL","text":"example, create binary annotation genes based gene locations (hg19). Step 01. Download gene location information GeneBreak package. Step 02. Create binary annotation genes. Notice: sHDL automatically align D LD reference panel according rsid, set weights missing variants LD reference panel ZERO.","code":"wget -c https://raw.githubusercontent.com/stefvanlieshout/GeneBreak/master/data/ens.gene.ann.hg19.rda load('ens.gene.ann.hg19.rda') # load ens.gene.ann.hg19 LD.path <- '/your/path/to/UKB_array_SVD_eigen90_extraction'  ## extract gene locations ## gene.pos <- unique(ens.gene.ann.hg19[, c(3:5)]) colnames(gene.pos) <- c('chrom', 'start', 'end') autosomes <- as.character(1:22) gene.pos <- gene.pos[gene.pos$chrom %in% autosomes, ] gene.pos$chrom <- as.numeric(gene.pos$chrom)  ## extract variants position from LD reference panel ## bim <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE )) bim <- bim[, c(2, 1, 4)] colnames(bim) <- c('SNP', 'chrom', 'pos')   ## match annotated variants ## gene2snp <- function(gene.pos, bim, mc.cores=4){   map.gene <- function(i, gene.pos.chr, bim.chr){     s <- gene.pos.chr[i, 'start']     e <- gene.pos.chr[i, 'end']     return(bim.chr[bim.chr$pos >= s & bim.chr$pos <= e, ]$SNP)   }   chroms <- unique(bim$chrom)   all.snps <- c()   for(cur.chrom in chroms){     gene.pos.chr <- gene.pos[gene.pos$chrom == cur.chrom, ]     bim.chr <- bim[bim$chrom == cur.chrom, ]     snps <- parallel::mclapply(       seq_len(nrow(gene.pos.chr)), map.gene,       gene.pos.chr=gene.pos.chr, bim.chr=bim.chr,       mc.cores=mc.cores     )     snps <- unique(unlist(snps))     all.snps <- c(all.snps, snps)   }   return(all.snps) } snps <- gene2snp(gene.pos, bim, 4)  ## create the annotation weights for sHDL ## D <- matrix(1, nrow=length(snps), ncol=1, dimnames=list(snps, 'ENS.gene'))  saveRDS(D, file='D.rds') # save the annotation weights for sHDL"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"run-shdl","dir":"Articles","previous_headings":"","what":"Run sHDL","title":"Example of sHDL","text":"sHDL finish ~2 mins 4 CPU cores. message console analysis (birth weight HDL) :","code":"library(sHDL)  LD.path <- '/your/path/to/UKB_array_SVD_eigen90_extraction' gwas.df <- readRDS('gwas.df.rds') D <- readRDS('D.rds')  t0 <- Sys.time() res <- sHDL::sHDL(D, gwas.df, LD.path, mc.cores=4, stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL) print(Sys.time() - t0) Starting sHDL analysis... 307519 out of 307519 (100.00%) SNPs in reference panel are available in the GWAS. Transfoming z (D) to zr (Dr)... D is not NULL and Dr.path is not provided. The default path ./sHDL_Dr will be used. Applied `minmax` weight nomalization on 147849 (48.078%) annotated variants. The theoretical upper boundary for enrichment fold is M / Md = 2.080. Starting optimization... Optimization done in 52.539 seconds. item    estimation      se      p       note time    52.5394132137299        NA      NA      seconds h2      0.16391651163241        0.00545604327753226     2.68604947190574e-198   total heritability intercept       0.963235890727237       0.0106589577723641      0       intercept fold.ENS.gene   1.2739805907671 0.0259721362754955      5.13295507071105e-26    enrichment fold converged       NA      NA      NA      TRUE message NA      NA      NA      CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL Time difference of 2.185567 mins"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"results-of-shdl","dir":"Articles","previous_headings":"","what":"Results of sHDL","title":"Example of sHDL","text":"Notice: item time indicates time maximizing log likelihood, including time converting z (D) zr (Dr). null hypothesis p value fold parameter \\(fold \\ne 1\\), \\(fold > 1\\).","code":""},{"path":"https://now2014.github.io/sHDL/articles/MultiGWAS.html","id":"step-01--convert-gwas-summary-statistics-to-mathbfz_r","dir":"Articles","previous_headings":"","what":"Step 01. Convert GWAS summary statistics to \\(\\mathbf{z}_r\\)","title":"Run sHDL across a large number of traits","text":"","code":"#!/usr/bin/env Rscript library(sHDL)  ## function to read GWAS Z-scores and sample size N from GWAS summary statistics ## read.gwas <- function(matZ, phe, gwas.dir=NULL, Ns=NULL, suffix='.sumstats.gz'){   gwas.file <- paste0(gwas.dir, '/', phe, suffix)   cat('Reading ', gwas.file, '\\n')   dfi <- data.table::fread(gwas.file, sep='\\t', header=TRUE)   snps <- dfi$SNP   N <- median(dfi$N, na.rm=TRUE)   Z <- matrix(dfi$Z, ncol=1)   ## add N to first row   Z <- rbind(N, Z)   row.names(Z) <- c('N', snps)    colnames(Z) <- phe   if(is.null(matZ)){     matZ <- Z   } else {     matZ <- cbind(matZ, Z)   }   return(matZ) }  convertZr.batch <- function(   phes, gwas.dir, zr.dir, LD.path, step=500, gwas.suffix='.sumstats.gz',   lam.cut=0.1, mc.cores=8, pattern='.*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*', overwrite=FALSE){    if(length(phes) == 0) return(NULL)   for(s in seq(1, length(phes), by=step)){     e <- min(s+step-1, length(phes))     Z <- reduce(phes[s:e], .f=read.gwas, gwas.dir=gwas.dir, suffix=gwas.suffix, .init=NULL)     N <- Z[1, ]     Z <- Z[-1, ]     N <- as.numeric(unlist(N))     names(N) <- colnames(Z)     ref.data <- sHDL::sHDL.reduct.dim(LD.path, z=Z, D=NULL, lam.cut=lam.cut, Dr.path=NULL,       overwrite=FALSE, mode='disk', mc.cores=mc.cores, pattern=pattern)     zr <- lapply(ref.data, function(x) x$zr)     rs <- unlist(lapply(zr, function(x) nrow(x)))      ## convert zr to list by phes     zr <- do.call(rbind, zr)     zr <- lapply(1:ncol(zr), function(i, rs=NULL, phes=NULL){       zz <- zr[, i]       e <- cumsum(rs)       s <- c(1, e[-length(e)]+1)       zz <- sapply(1:length(s), function(j) zz[s[j]:e[j]])       return(zz)     }, rs=rs, phes=colnames(Z))     names(zr) <- colnames(Z)     for(phe in names(zr)){       cur.zr <- list()       cur.zr$zr <- zr[[phe]]       cur.zr$N <- unname(unlist(N[phe]))       saveRDS(cur.zr, file=paste0(zr.dir, '/', phe, '.rds'))     }   } }  overwrite <- FALSE LD.path <- '/path/to/UKB_array_SVD_eigen90_extraction' lam.cut <- 1 mc.cores <- 8 step <- 500 gwas.dir <- '/path/to/gwas/sumstats' gwas.suffix <- '.sumstats.gz' zr.dir <- '/path/to/store/zr' pattern <- '.*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*'  phes <- gsub(gwas.suffix, '', list.files(gwas.dir, pattern=gwas.suffix)) if(!dir.exists(zr.dir)) dir.create(zr.dir, recursive=TRUE) exist.phes <- gsub('\\\\.rds', '', list.files(zr.dir, pattern='*.rds')) if(!overwrite) phes <- setdiff(phes, exist.phes) cat('Converting', length(phes), 'traits...\\n')  #### convert z to zr #### convertZr.batch(phes, gwas.dir, zr.dir, LD.path, step=step, gwas.suffix=gwas.suffix,                 lam.cut=lam.cut, mc.cores=mc.cores, pattern=pattern, overwrite=overwrite)"},{"path":"https://now2014.github.io/sHDL/articles/MultiGWAS.html","id":"step-02--convert-genomic-annotations-to-mathbfd_r","dir":"Articles","previous_headings":"","what":"Step 02. Convert genomic annotations to \\(\\mathbf{D}_r\\)","title":"Run sHDL across a large number of traits","text":"","code":"#!/usr/bin/env Rscript  read.annot <- function(ann.name, annot.dir){   ## write this function to return the D matrix of given ann.name   return(D) }  create.D <- function(   annot.names, LD.path, annot.dir,   pattern='.*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*'){    D <- NULL   for(ann.name in annot.names){     dD <- read.annot(ann.name, annot.dir)     dD <- sHDL:::normD(dD, LD.path, norm.method = 'none', pattern = pattern)     D <- cbind(D, dD)   }   return(D) }  annot.dir <- '/path/to/directory/of/annotations' lam.cut <- 1 Dr.path <- './sHDL_Dr_lam_1' overwrite <- FALSE mc.cores <- 8 pattern <- '.*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*' norm.method <- 'minmax' annot.names <- c('all', 'your', 'annot', 'names')  ## read D matrix D <- create.D(annot.names, LD.path, annot.dir, pattern)  ## convert D to Dr ref.data <- sHDL::sHDL.reduct.dim(   LD.path, z=NULL, D=D, lam.cut=lam.cut, Dr.path=Dr.path, overwrite=overwrite,   mode='disk', mc.cores=mc.cores, pattern=pattern, norm.method=norm.method )"},{"path":"https://now2014.github.io/sHDL/articles/MultiGWAS.html","id":"step-03--run-shdl-in-parallel","dir":"Articles","previous_headings":"","what":"Step 03. Run sHDL in parallel","title":"Run sHDL across a large number of traits","text":"","code":"run.sHDL <- function(phe, zr.dir, res.dir, ref.data, overwrite=FALSE){   tsv.file <- paste0(res.dir, '/', phe, '.tsv')   log.file <- paste0(res.dir, '/', phe, '.log')   if(file.exists(tsv.file) && !overwrite) return(NULL)   if(file.exists(log.file)) file.remove(log.file)    zr.N <- readRDS(paste0(zr.dir, '/', phe, '.rds'))   N <- zr.N$N   for(i in seq_len(length(ref.data))){     rank <- length(ref.data[[i]]$lam)     ref.data[[i]]$zr <- zr.N$zr[[i]][seq_len(rank)]   }   res <- sHDL::sHDL.optim(ref.data, N, output.file=tsv.file,     log.file=log.file, stepwise = TRUE, verbose=TRUE   )   return(res) }  overwrite <- FALSE zr.dir <- '/path/to/store/zr' Dr.path <- './sHDL_Dr_lam_1' out.dir <- './res-sHDL' mc.cores <- 8 mode <- 'memory' lam.cut <- 1 pattern <- '.*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*'  annot.names <- list.dirs(Dr.path, recursive = FALSE, full.names=FALSE) phes <- list.files(zr.dir, '*.rds$', recursive = FALSE, full.names=FALSE) phes <- gsub('\\\\.rds$', '', phes)  for(ann.name in annot.names){   res.dir <- paste0(out.dir, '/', ann.name)   if(!dir.exists(res.dir)) dir.create(res.dir, recursive = TRUE)   exist.phes <- gsub('\\\\.tsv$', '', list.files(res.dir, pattern='*.tsvs'))   if(!overwrite) phes <- setdiff(phes, exist.phes)   if(length(phes) < 1) next    ref.data <- sHDL:::load.Dr(Dr.path, ann.name, lam.cut=lam.cut,     pattern=pattern, mc.cores=mc.cores, mode=mode)   tmp <- parallel::mclapply(     phes, run.sHDL, zr.dir, res.dir, ref.data, mc.cores=mc.cores   ) }"},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"missing-snps-warning","dir":"Articles","previous_headings":"","what":"Missing SNPs warning","title":"Mismatched LD reference panel","text":"matched LD reference panel vital analysis GWAS summary statistics. running sHDL (HDL), may encounter following warning message: default, sHDL (HDL) fills GWAS z-scores zeros SNPs LD reference panel missing.","code":"Warning: More than 1% SNPs in reference panel are missed ... This may generate bias in estimation. Please make sure that you are using correct reference panel."},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"solution","dir":"Articles","previous_headings":"","what":"Solution","title":"Mismatched LD reference panel","text":"address issue, provide function rebuild LD reference panel removing missed SNPs. Caution: GWAS summary statistics severe missingness, heritability may severely underestimated efficiency might lost. rebuilding process resolve mismatch cross-ancestry LD structures.","code":""},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Mismatched LD reference panel","text":"Suppose gone tutorial prepared LD reference panel, installed sHDL HDL packages. example, demo QCed UK Biobank Axiom Array (307,519 SNPs) reference panel.","code":""},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"simulate-mismatched-gwas-snps","dir":"Articles","previous_headings":"Example","what":"Simulate mismatched GWAS SNPs","title":"Mismatched LD reference panel","text":"simulate mismatched GWAS SNPs, randomly subset 80% SNPs LD reference panel.","code":"library(sHDL) LD.path <- '/your/path/to/UKB_array_SVD_eigen90_extraction' all.snps <- do.call(rbind, lapply(sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), read.table, header=FALSE))[, 2] set.seed(1234) gwas.snps <- sample(all.snps, size=floor(length(all.snps) * 0.8), replace=FALSE)"},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"re-build-the-ld-reference-panel","dir":"Articles","previous_headings":"Example","what":"Re-build the LD reference panel","title":"Mismatched LD reference panel","text":"rebuild LD reference panel gwas.snps store new LD reference panel /path//new/UKB_array_SVD_eigen90_extraction. process may take , can put following code script run background nohup. message console process:","code":"LD.path.new <- '/path/to/new/UKB_array_SVD_eigen90_extraction' sHDL.rebuild.ref(LD.path, gwas.snps, LD.path.new, nthreads = 8) # run with 8 CPU cores Cleaning the new directory... Updating chr1_1 in 21.80 seconds: 6477 (out of 8090) SNPs are kept. Updating chr1_2 in 21.22 seconds: 6468 (out of 8090) SNPs are kept. Updating chr1_3 in 21.69 seconds: 6493 (out of 8090) SNPs are kept. Updating chr2_1 in 21.14 seconds: 6502 (out of 8156) SNPs are kept. Updating chr2_2 in 21.81 seconds: 6558 (out of 8156) SNPs are kept. Updating chr2_3 in 22.30 seconds: 6487 (out of 8154) SNPs are kept. Updating chr3_1 in 15.25 seconds: 5573 (out of 6935) SNPs are kept. Updating chr3_2 in 14.38 seconds: 5502 (out of 6935) SNPs are kept. Updating chr3_3 in 14.37 seconds: 5524 (out of 6935) SNPs are kept. Updating chr4_1 in 37.38 seconds: 7905 (out of 9777) SNPs are kept. Updating chr4_2 in 37.14 seconds: 7870 (out of 9776) SNPs are kept. Updating chr5_1 in 31.47 seconds: 7363 (out of 9260) SNPs are kept. Updating chr5_2 in 31.76 seconds: 7396 (out of 9260) SNPs are kept. Updating chr6_1 in 3.75 seconds: 3054 (out of 3825) SNPs are kept. Updating chr6_2 in 13.50 seconds: 5386 (out of 6757) SNPs are kept. Updating chr6_3 in 14.17 seconds: 5431 (out of 6756) SNPs are kept. Updating chr7_1 in 25.18 seconds: 6931 (out of 8576) SNPs are kept. Updating chr7_2 in 27.01 seconds: 6880 (out of 8575) SNPs are kept. Updating chr8_1 in 20.67 seconds: 6347 (out of 7954) SNPs are kept. Updating chr8_2 in 21.24 seconds: 6389 (out of 7953) SNPs are kept. Updating chr9_1 in 15.26 seconds: 5575 (out of 6949) SNPs are kept. Updating chr9_2 in 14.73 seconds: 5579 (out of 6948) SNPs are kept. Updating chr10_1 in 19.80 seconds: 6200 (out of 7742) SNPs are kept. Updating chr10_2 in 20.10 seconds: 6138 (out of 7741) SNPs are kept. Updating chr11_1 in 17.51 seconds: 5962 (out of 7513) SNPs are kept. Updating chr11_2 in 17.70 seconds: 6038 (out of 7513) SNPs are kept. Updating chr12_1 in 17.84 seconds: 5964 (out of 7470) SNPs are kept. Updating chr12_2 in 17.99 seconds: 5983 (out of 7469) SNPs are kept. Updating chr13_1 in 8.01 seconds: 4386 (out of 5495) SNPs are kept. Updating chr13_2 in 8.21 seconds: 4433 (out of 5495) SNPs are kept. Updating chr14_1 in 7.03 seconds: 4076 (out of 5094) SNPs are kept. Updating chr14_2 in 6.39 seconds: 3979 (out of 5094) SNPs are kept. Updating chr15_1 in 6.50 seconds: 4026 (out of 5023) SNPs are kept. Updating chr15_2 in 6.45 seconds: 4021 (out of 5023) SNPs are kept. Updating chr16_1 in 8.47 seconds: 4494 (out of 5592) SNPs are kept. Updating chr16_2 in 8.46 seconds: 4472 (out of 5591) SNPs are kept. Updating chr17_1 in 6.92 seconds: 4123 (out of 5172) SNPs are kept. Updating chr17_2 in 6.98 seconds: 4115 (out of 5171) SNPs are kept. Updating chr18_1 in 38.63 seconds: 7818 (out of 9783) SNPs are kept. Updating chr19_1 in 26.94 seconds: 6991 (out of 8711) SNPs are kept. Updating chr20_1 in 25.60 seconds: 6797 (out of 8512) SNPs are kept. Updating chr21_1 in 6.81 seconds: 3957 (out of 4988) SNPs are kept. Updating chr22_1 in 8.26 seconds: 4352 (out of 5420) SNPs are kept. [1] \"/path/to/new/UKB_array_SVD_eigen90_extraction\""},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"test-the-new-ld-reference-panel","dir":"Articles","previous_headings":"Example","what":"Test the new LD reference panel","title":"Mismatched LD reference panel","text":"Test sHDL::sHDL Test HDL::HDL.h2","code":"data(gwas1.example, package='HDL') M <- nrow(gwas1.example) set.seed(1234) D <- rbinom(M, 1, 0.1) # random D vector names(D) <- gwas1.example$SNP  res.sHDL <- sHDL(   D, gwas1.example, LD.path.new, nthreads=4,   stepwise=TRUE, lam.cut=1, Dr.path=NULL, mode=\"memory\" ) # Starting sHDL analysis... # 246015 out of 246015 (100.00%) SNPs in reference panel are available in the GWAS. # Transfoming z (D) to zr (Dr)... # D is not NULL and Dr.path is not provided. The default path ./sHDL_Dr will be used. # Annotation name(s): [randD]. # Applied `minmax` weight nomalization on [24634 (10.013%)] annotated variants. The theoretical upper boundary for enrichment fold is M / Md = [9.987]. # Starting optimization... # Optimization done in 41.753 seconds. # item    estimation      se      p       note # time    41.7533082962036        NA      NA      seconds # h2      0.150313183379785       0.00564781178171049     4.62367581854552e-156   total heritability # intercept       0.985128189916525       0.0126279452459156      0       intercept # fold.randD      0.89182953485042        0.12523419788762        0.387727755380332       enrichment fold # converged       NA      NA      NA      TRUE # message NA      NA      NA      CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL data(gwas1.example, package='HDL') HDL::HDL.h2(gwas1.example, LD.path.new)  # 246015 out of 246015 (100%) SNPs in reference panel are available in the GWAS. # Estimation is ongoing ... 100%  # Integrating piecewise results  # Point estimate: # Heritability:  0.1407  # Continuing computing standard error with jackknife # Progress... 100%  # Heritability:  0.1407 (0.0073) # P:  8.09e-82  # $h2 # [1] 0.1407243  # $h2.se # [1] 0.007344952  # $P # [1] 8.093035e-82  # $eigen.use # [1] 0.9"},{"path":"https://now2014.github.io/sHDL/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ao Lan. Author, maintainer. Xia Shen. Author.","code":""},{"path":"https://now2014.github.io/sHDL/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lan . Shen X. Modeling genetic architecture complex traits via stratified high-definition likelihood. (2024).","code":"@Article{,   title = {Modeling the genetic architecture of complex traits via stratified high-definition likelihood},   author = {Ao Lan and Xia Shen},   journal = {.},   year = {2024},   volume = {0},   number = {0},   pages = {0-0}, }"},{"path":[]},{"path":"https://now2014.github.io/sHDL/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"sHDL software provides powerful tool analyzing genetic architecture complex traits distribution heritability across genome. integrating state---art statistical techniques genomic functional annotations, sHDL offers comprehensive approach understanding genetic basis complex traits. sHDL builds upon success stratified linkage disequilibrium score regression (sLDSC) method introducing new stratified high-definition likelihood model. advanced statistical model enhances estimation efficiency heritability enrichment parameters reducing bias, demonstrated simulations real-data analyses. Compared sLDSC, sHDL offers 1.4 7.4-fold higher efficiency estimating heritability enrichment parameters. One key features sHDL ability incorporate diverse genomic annotations, including gene expressions specific cell types cancer epigenetic information. enables identification specific genomic regions cell types associated complex traits, leading deeper understanding genetic architecture. extensive validation comparison sLDSC, sHDL shown discover novel signals provide accurate estimations heritability enrichment parameters. makes sHDL robust versatile tool researchers field quantitative genetics, offering new insights polygenic contributions complex traits.","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"install latest version sHDL package via Github, run following code R:","code":"# install.packages('remotes') remotes::install_github(\"now2014/sHDL\", ref='main')"},{"path":"https://now2014.github.io/sHDL/index.html","id":"quick-vignette","dir":"","previous_headings":"","what":"Quick vignette","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"Download LD reference panel HDL software: Run sHDL","code":"wget -c -t 1 \\   https://www.dropbox.com/s/fuvpwsf6r8tjd6c/UKB_array_SVD_eigen90_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_array_SVD_eigen90_extraction.tar.gz  # checking the md5 = ff3fadd7ea08bd29759b6c652618cd1f md5sum /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz  # extraction tar -xvf /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz remotes::install_github(\"zhenin/HDL/HDL\") data(gwas1.example, package=\"HDL\")  library(sHDL)  ## The GWAS summary statistics for birth weight loaded from HDL package. M <- nrow(gwas1.example) set.seed(1234) D <- rbinom(M, 1, 0.01) # random D vector names(D) <- gwas1.example$SNP  ## The path to the directory where linkage disequilibrium (LD) information is stored. LD.path <- \"/your/path/to/UKB_array_SVD_eigen90_extraction\"  ## To speed up the test, we set a large lam.cut value. res.sHDL <- sHDL(D, gwas1.example, LD.path, nthreads=4, stepwise=TRUE, lam.cut=10, Dr.path=NULL, mode=\"memory\") print(as.data.frame(res.sHDL))"},{"path":"https://now2014.github.io/sHDL/index.html","id":"more-tutorials","dir":"","previous_headings":"","what":"More tutorials","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"detail please see tutorials https://now2014.github.io/sHDL/.","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"use sHDL software, please cite: Lan . Shen X. Modeling genetic architecture complex traits via stratified high-definition likelihood. (2024). Ning, Z., Pawitan, Y. & Shen, X. High-definition likelihood inference genetic correlations across human complex traits. Nat Genet (2020).","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"contact","dir":"","previous_headings":"","what":"Contact","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"Reporting bugs opening new issue Github page. Sending email authors: Ao Lan Xia Shen.","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"future-plan","dir":"","previous_headings":"","what":"Future plan","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"repository integrated HDL software future.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Perform heritability enrichment analysis trait based GWAS summary statistics.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"","code":"sHDL(   D,   gwas.df,   LD.path,   output.file = NULL,   mc.cores = 1,   stepwise = TRUE,   fill.missing.N = c(\"none\", \"min\", \"max\", \"median\", \"mean\"),   lim = exp(-18),   lam.cut = NULL,   Dr.path = \"./Dr\",   overwrite = FALSE,   verbose = FALSE,   fix.h2 = NULL,   fix.intercept = NULL,   maxit = 1000,   pgtol = 0.001,   start.v = c(0.1, 1, 1),   lwr = NULL,   upr = NULL,   mode = c(\"disk\", \"memory\"),   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\",   norm.method = c(\"minmax\", \"scaled\", \"none\"),   par.h2 = FALSE,   nthreads = 1 )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"D single-column matrix annotation weights rownames SNP IDs colname specifying annotation name. Must exactly one column. gwas.df data frame including GWAS summary statistics genetic variants trait. input data frame include following columns: SNP, SNP ID; A1, effect allele; A2, reference allele; N, sample size; Z, z-score; Z given, alternatively, may provide: b, estimate marginal effect GWAS; se, standard error estimates marginal effects GWAS. LD.path Path directory linkage disequilibrium (LD) information stored, compatible HDL LD reference panels. output.file log results written. specify file, log printed console. mc.cores Number cores use parallelization, default mc.cores = 1. stepwise Whether estimate enrichment fold estimating heritability intercept first, default stepwise = FALSE. fix.h2 fix.intercept NULL, stepwise overridden. fill.missing.N sample size missing GWAS summary statistics, fill.missing.N used fill missing values. lim Tolerance limitation ensure positive-definiteness covariance matrices, default lim = exp(-18). lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff. analyses limited number traits annotations, lower cutoff (0.1, even using cutoff ) recommended. large-scale analyses, higher cutoff (1)  recommended, yield fast computation. Dr.path Path directory Dr matrices stored, default Dr.path = \"./Dr\". overwrite Whether overwrite existing Dr matrices, default overwrite = FALSE. verbose Whether print log console, default verbose = FALSE. fix.h2 Whether fix heritability fix.h2 estimate heritability, default fix.h2 = NULL, means estimate heritability. fix.intercept Whether fix intercept fix.intercept estimate intercept, default fix.intercept = NULL, means estimate intercept. maxit Maximum number iterations, default maxit = 1000. pgtol Tolerance convergence, default pgtol = 1e-3. start.v vector starting values c(h2, intercept, fold) optimization. lwr Lower bounds c(h2, intercept, fold). Default NULL, means c(0, 0.1, 0). upr Upper bounds c(h2, intercept, fold). Default NULL, means c(1, 5, M / Md), Md sum annotation weights M total number SNPs. mode mode = \"disk\", Dr stored disk (path returned ). mode = \"memory\", Dr loaded memory (matrix returned). Default mode = \"disk\". pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\". norm.method normalization method, either \"minmax\" (default), \"scaled\" \"none\". \"minmax\", annotation weight vector D normalized [0, 1]. \"scaled\", sum normalized vector D scaled number annotated SNPs. \"none\", annotation weight vector D normalized. par.h2 Whether estimate partitioned heritability, default par.h2 = FALSE. nthreads Number threads use matrix operations, default nthreads = 1. default value suitable cases, change unless sure performance.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"data.frame returned : item  name parameter. estimation  estimated value parameter. se  standard error parameter. p  p-value parameter. note  note parameter.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Users can download precomputed eigenvalues eigenvectors LD correlation matrices European ancestry population. download link can found https://github.com/zhenin/HDL/wiki/Reference-panels LD matrices eigen-decomposition 335,265 genomic British UK Biobank individuals. Three sets reference panel provided: 1) 1,029,876 QCed UK Biobank imputed HapMap3 SNPs. size 33 GB unzipping. Although takes time, using imputed panel provides accurate estimates genetic correlations. Therefore GWAS includes HapMap3 SNPs, recommend using imputed reference panel. 2) 769,306 QCed UK Biobank imputed HapMap2 SNPs. size 18 GB unzipping.one GWAS includes HapMap 2 SNPs, many SNPs (1 HapMap2 panel proper used HDL. 3) 307,519 QCed UK Biobank Axiom Array SNPs. size 7.5 GB unzipping.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Lan Shen X (2024). Modeling Genetic Architecture Complex Traits via Stratified High-Definition Likelihood.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Ao Lan, Xia Shen","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"","code":"if (FALSE) { ## The GWAS summary statistics for birth weight loaded from HDL package. data(gwas1.example, package='HDL') M <- nrow(gwas1.example) set.seed(1234) D <- matrix(rbinom(M, 1, 0.01), ncol=1) # random D vector row.names(D) <- gwas1.example$SNP colnames(D) <- 'testAnno'  ## The path to the directory where linkage disequilibrium (LD) information is stored. LD.path <- \"path/to/UKB_array_SVD_eigen90_extraction\"  ## To speed up the test, we set a large lam.cut value. res.sHDL <- sHDL(D, gwas1.example, LD.path, mc.cores=4, stepwise=TRUE, lam.cut=10, Dr.path=NULL, mode=\"memory\") print(as.data.frame(res.sHDL)) }"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.joint.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximize the log likelihood to infer the heritability enrichment folds jointly. — sHDL.joint","title":"Maximize the log likelihood to infer the heritability enrichment folds jointly. — sHDL.joint","text":"Maximize log likelihood infer heritability enrichment folds jointly.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.joint.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximize the log likelihood to infer the heritability enrichment folds jointly. — sHDL.joint","text":"","code":"sHDL.joint(   Dr.path,   gwas.df,   LD.path,   output.file = NULL,   annots = NULL,   mc.cores = 1,   fill.missing.N = c(\"none\", \"min\", \"max\", \"median\", \"mean\"),   lim = exp(-18),   lam.cut = NULL,   verbose = FALSE,   fix.h2 = NULL,   fix.intercept = NULL,   maxit = 1000,   pgtol = 0.001,   start.v = NULL,   lwr = NULL,   upr = NULL,   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\",   par.h2 = FALSE,   nthreads = 1 )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.joint.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximize the log likelihood to infer the heritability enrichment folds jointly. — sHDL.joint","text":"Dr.path path pre-computed diag(Dr) data, see also sHDL::sHDL.joint.reduct.dim. gwas.df data frame including GWAS summary statistics genetic variants trait. input data frame include following columns: SNP, SNP ID; A1, effect allele; A2, reference allele; N, sample size; Z, z-score; Z given, alternatively, may provide: b, estimate marginal effect GWAS; se, standard error estimates marginal effects GWAS. LD.path Path directory linkage disequilibrium (LD) information stored, compatible HDL LD reference panels. output.file log results written. specify file, log printed console. annots vector annotation names, default NULL, means annotations Dr.path used. mc.cores Number cores use parallelization, default mc.cores = 1. fill.missing.N sample size missing GWAS summary statistics, fill.missing.N used fill missing values. lim Tolerance limitation ensure positive-definiteness covariance matrices, default lim = exp(-18). lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff. analyses limited number traits annotations, lower cutoff (0.1, even using cutoff ) recommended. large-scale analyses, higher cutoff (1)  recommended, yield fast computation. verbose Whether print log console, default verbose = FALSE. fix.h2 Whether fix heritability fix.h2 estimate heritability, default fix.h2 = NULL, means estimate heritability. fix.intercept Whether fix intercept fix.intercept estimate intercept, default fix.intercept = NULL, means estimate intercept. maxit Maximum number iterations, default maxit = 1000. pgtol Tolerance convergence, default pgtol = 1e-3. start.v Starting values c(h2, intercept, fold1, fold2, ..., foldn), n number annotations. Default NULL, means c(0.1, 1, 1, ..., 1). fix.h2 fix.intercept NULL, starting values h2 intercept ignored. lwr Lower bounds c(h2, intercept, fold1, fold2, ..., foldn). Default NULL, means c(0, 0, 0, 0, ..., 0). fix.h2 fix.intercept NULL. fix.h2 fix.intercept NULL, lower bounds h2 intercept ignored. upr Upper bounds c(h2, intercept, fold1, fold2, ..., foldn), default NULL, means c(1, 5, M/Md_1, M/Md_2, ..., M/Md_n), Md_i sum weights -th annotation. fix.h2 fix.intercept NULL, upper bounds h2 intercept ignored. pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\". par.h2 Whether estimate partitioned heritability, default par.h2 = FALSE. nthreads Number threads use matrix operations, default nthreads = 1. default value suitable cases, change unless sure performance.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.joint.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximize the log likelihood to infer the heritability enrichment folds jointly. — sHDL.joint","text":"data.frame returned : item  name parameter. estimation  estimated value parameter. se  standard error parameter. p  p-value parameter. note  note parameter.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximize the log likelihood to infer the heritability enrichment fold(s). — sHDL.optim","title":"Maximize the log likelihood to infer the heritability enrichment fold(s). — sHDL.optim","text":"Maximize log likelihood infer heritability enrichment fold(s).","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximize the log likelihood to infer the heritability enrichment fold(s). — sHDL.optim","text":"","code":"sHDL.optim(   ref.data,   N,   start.v = NULL,   output.file = NULL,   log.file = \"\",   stepwise = TRUE,   fix.h2 = NULL,   fix.intercept = NULL,   lim = exp(-18),   verbose = FALSE,   lwr = NULL,   upr = NULL,   maxit = 1000,   pgtol = 0.001,   par.h2 = FALSE,   mc.cores = 1,   nthreads = 1 )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximize the log likelihood to infer the heritability enrichment fold(s). — sHDL.optim","text":"ref.data list data matched LD reference generated sHDL.joint.reduct.dim. N sample size GWAS. start.v Starting values c(h2, intercept, fold1, fold2, ..., foldn), n number annotations. Default NULL, means c(0.1, 1, 1, ..., 1). fix.h2 fix.intercept NULL, starting values h2 intercept ignored. output.file results written. log.file log written. stepwise Whether estimate enrichment fold estimating heritability intercept first, default stepwise = FALSE. fix.h2 fix.intercept NULL, stepwise overridden. fix.h2 Whether fix heritability fix.h2 estimate heritability, default fix.h2 = NULL, means estimate heritability. fix.intercept Whether fix intercept fix.intercept estimate intercept, default fix.intercept = NULL, means estimate intercept. lim Tolerance limitation ensure positive-definiteness covariance matrices, default lim = exp(-18). verbose Whether print log console, default verbose = FALSE. lwr Lower bounds c(h2, intercept, fold1, fold2, ..., foldn). Default NULL, means c(0, 0, 0, 0, ..., 0). fix.h2 fix.intercept NULL. fix.h2 fix.intercept NULL, lower bounds h2 intercept ignored. upr Upper bounds c(h2, intercept, fold1, fold2, ..., foldn), default NULL, means c(1, 5, M/Md_1, M/Md_2, ..., M/Md_n), Md_i sum weights -th annotation. fix.h2 fix.intercept NULL, upper bounds h2 intercept ignored. maxit Maximum number iterations, default maxit = 1000. pgtol Tolerance convergence, default pgtol = 1e-3. par.h2 Whether estimate partitioned heritability, default par.h2 = FALSE. mc.cores Number cores use parallelization, default mc.cores = 1. nthreads Number threads use matrix operations, default nthreads = 1. default value suitable cases, change unless sure performance. annots vector annotation names, default NULL, means annotations Dr.path used. lam.cut Cutoff eigenvalues, default NULL, means cutoff. analyses limited number traits annotations, lower cutoff (0.1, even using cutoff ) recommended. large-scale analyses, higher cutoff (1)  recommended, yield fast computation.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximize the log likelihood to infer the heritability enrichment fold(s). — sHDL.optim","text":"data.frame returned : item  name parameter. estimation  estimated value parameter. se  standard error parameter. p  p-value parameter. note  note parameter.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":null,"dir":"Reference","previous_headings":"","what":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"Re-build LD reference panel given SNPs.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"","code":"sHDL.rebuild.ref(   LD.path,   gwas.snps,   LD.path.new,   nthreads = 1,   lam.cut = NULL,   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\" )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"LD.path Path .rda file Eigen decomposition LD matrix stored. gwas.snps vector contains SNP IDs GWAS summary statistics. LD.path.new new path re-built LD reference. nthreads Number threads use, default nthreads = 1. lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff original LD reference panel. analyses limited number traits annotations, lower cutoff (0.1, even using cutoff ) recommended. large-scale analyses, higher cutoff (1)  recommended, yield fast computation. pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\".","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"new path re-built LD reference.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"Reduce dimension covariance matrix converting z (D) zr (Dr).","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"","code":"sHDL.reduct.dim(   LD.path,   z = NULL,   D = NULL,   lam.cut = NULL,   Dr.path = NULL,   overwrite = FALSE,   mode = c(\"disk\", \"memory\"),   mc.cores = 1,   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\",   norm.method = c(\"minmax\", \"scaled\", \"none\"),   log.file = \"\",   nthreads = 1 )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"LD.path Path .rda file Eigen decomposition LD matrix stored. z matrix Z-scores rownames SNP IDs. Supporting multiple columns multiple traits. D matrix annotation weights rownames SNP IDs colnames specifying annotation names. lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff. analyses limited number traits annotations, lower cutoff (0.1, even using cutoff ) recommended. large-scale analyses, higher cutoff (1)  recommended, yield fast computation. Dr.path Path directory Dr matrices stored, default Dr.path = NULL, means store Dr disk. overwrite Whether overwrite existing Dr matrices, default overwrite = FALSE. mode Whether store Dr disk memory, default mode = \"disk\". mode = \"disk\", Dr stored disk (path returned ) lam returned. mode = \"memory\", Dr lam returned. mc.cores Number cores use parallelization, default mc.cores = 1. pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\". norm.method normalization method, either \"minmax\" (default), \"scaled\" \"none\". \"minmax\", annotation weight vector D normalized [0, 1]. \"scaled\", sum normalized vector D scaled number annotated SNPs. \"none\", annotation weight vector D normalized. log.file log written. specify file, log printed console. nthreads Number threads use matrix operations, default nthreads = 1. default value suitable cases, change unless sure performance.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"list returned : Dr reduct RDR matrix. zr reduct z-score vector. lam eigenvalues LD matrix. Md sum annotation weights SNPs given LD matrix. M number SNPs given LD matrix.","code":""}]
