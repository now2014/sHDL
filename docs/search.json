[{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"inroduction","dir":"Articles","previous_headings":"","what":"Inroduction","title":"Genomic annotations","text":"marks significant breakthrough sHDL integrates interpretation framework binary continuous genomic annotations. tutorial illustrate process estimating heritability enrichment fold following acquisition genomic features. !!!Caution!!!: weights different annotations aren’t comparable, direct cross-annotation comparisons enrichment folds avoided. cases, comparing p-values enrichment fold estimation appropriate.","code":""},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Genomic annotations","text":"Suppose gone tutorial prepared LD reference panel: UKB_array_SVD_eigen90_extraction, downloaded gene locations information GeneBreak package: ens.gene.ann.hg19.rda, installed sHDL HDL packages. Additionally, download gene specificity data brain cell types (Skene et al. 2017) hjerling leffler lab. Data contents tutorial: use gene specificity ASC cell type (Astrocytes) annotation ctd_DRONC_human.rda file illustration.","code":"wget -c http://www.hjerling-leffler-lab.org/data/scz_singlecell/ctdFiles.zip  unzip ctdFiles.zip tree ./                                 # ./ # ├── ctd_AIBS.rda # ├── ctd_allKI.rda # ├── ctd_DRONC_human.rda # ├── ctd_DRONC_mouse.rda # ├── ens.gene.ann.hg19.rda  # gene locations # └── UKB_array_SVD_eigen90_extraction  # LD reference panel load('ctd_DRONC_human.rda') print(ctd[[1]]$specificity[1:5, 1:5]) #                  ASC       END        exCA        exDG       exPFC # A1BG     0.065780027 0.0000000 0.134620574 0.108029999 0.073845381 # A1BG-AS1 0.021622098 0.1625062 0.168150754 0.113631342 0.197219791 # A1CF     0.064952275 0.2440825 0.053170623 0.000000000 0.000000000 # A2M      0.003421627 0.8803978 0.006920069 0.005024324 0.004067106 # A2M-AS1  0.035128297 0.3960231 0.028756397 0.069229097 0.083800251  gene.weights <- ctd[[1]]$specificity[, 'ASC']  summary(gene.weights) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  # 0.00000 0.02942 0.05097 0.06333 0.07369 1.00000"},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"binary-annotation","dir":"Articles","previous_headings":"","what":"Binary annotation","title":"Genomic annotations","text":"Step 01. create vector D containing annotated SNPs located 10% genes highest specificity scores. Step 02. Run sHDL binary annotation birth weight GWAS summary statistics HDL.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction'  ## extract the top 10% genes ## q90 <- quantile(gene.weights, 0.9) top10.genes <- names(gene.weights)[gene.weights > q90]  ## match gene locations ## load('ens.gene.ann.hg19.rda') # load ens.gene.ann.hg19 idx <- ens.gene.ann.hg19$Gene %in% top10.genes gene.pos <- unique(ens.gene.ann.hg19[idx, c(3:5)]) colnames(gene.pos) <- c('chrom', 'start', 'end') autosomes <- as.character(1:22) gene.pos <- gene.pos[gene.pos$chrom %in% autosomes, ] # keep autosomes only gene.pos$chrom <- as.numeric(gene.pos$chrom)  ## extract variants position from LD reference panel ## bim <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE )) bim <- bim[, c(2, 1, 4)] colnames(bim) <- c('SNP', 'chrom', 'pos')  ## match annotated variants ## gene2snp <- function(i, j){   cur.chrom <- gene.pos[j, 'chrom']   s <- gene.pos[j, 'start']   e <- gene.pos[j, 'end']   bim <- bim[bim$chrom == cur.chrom, ]   bim <- bim[bim$pos >= s & bim$pos <= e, ]   snps <- unique(c(i, bim$SNP))   nsnps <- length(snps)   if(j %% 1000 == 0) cat(sprintf(     '# of processed genes %d: # of annotated %d variants (%.2f%%) \\n',     j, nsnps, nsnps/M*100   ))   return(snps) } M <- nrow(bim) snps <- Reduce(gene2snp, 1:nrow(gene.pos), init=c())  ## create the annotation weights for sHDL ## D <- rep(1, length(snps)) names(D) <- snps data(gwas1.example, package='HDL') res <- sHDL::sHDL(   D, gwas1.example, LD.path, nthreads=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL )  str(res) # List of 13 #  $ time         : num 78.8 #  $ fold         : num 1.3 #  $ h2           : num 0.164 #  $ intercept    : num 0.963 #  $ fold.se      : num 0.105 #  $ h2.se        : num 0.00546 #  $ intercept.se : num 0.0107 #  $ fold.p       : num 0.00374 #  $ h2.p         : num 2.69e-198 #  $ intercept.p  : num 0 #  $ stepwise     : logi TRUE #  $ converged    : logi TRUE #  $ optim.message: chr \"CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL\""},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"continuous-annotation","dir":"Articles","previous_headings":"","what":"Continuous annotation","title":"Genomic annotations","text":"context continuous annotation, normalization process employed ensure annotation fits sHDL model. Step 01. Extract gene locations (hg19) weights. Step 02. Map gene weights SNP weights. Step 03. Check weights variants.","code":"## extract gene infomation ## load('ens.gene.ann.hg19.rda') # load ens.gene.ann.hg19 gene.df <- unique(ens.gene.ann.hg19[, c(1, 3:5)]) colnames(gene.df) <- c('gene', 'chrom', 'start', 'end') autosomes <- as.character(1:22) gene.df <- gene.df[gene.df$chrom %in% autosomes, ] # keep autosomes only gene.df$chrom <- as.numeric(gene.df$chrom) gene.df <- gene.df[!duplicated(gene.df$gene), ] # remove duplicated genes gene.df$weight <- gene.weights[gene.df$gene] gene.df <- gene.df[!is.na(gene.df$weight), ] # remove genes without weight  head(gene.df, 5) #            gene chrom  start    end     weight # 27828 LINC00115     1 761586 762902 0.13639639 # 12129    SAMD11     1 860260 879955 0.52717077 # 12571     NOC2L     1 879584 894689 0.05715007 # 12699    KLHL17     1 895967 901095 0.07142173 # 13025      HES4     1 934342 935552 0.09547616 LD.path <- 'UKB_array_SVD_eigen90_extraction'  ## extract variants position from LD reference panel ## bim <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE )) bim <- bim[, c(2, 1, 4)] colnames(bim) <- c('SNP', 'chrom', 'pos')  ## map weights ## mapw <- function(i, j){   cur.chrom <- gene.df[j, 'chrom']   s <- gene.df[j, 'start']   e <- gene.df[j, 'end']   w <- gene.df[j, 'weight']   bim <- bim[bim$chrom == cur.chrom, ]   bim <- bim[bim$pos >= s & bim$pos <= e, ]    snps <- unique(c(names(i), bim$SNP))   nsnps <- length(snps)   snps.w <- rep(0, nsnps)   names(snps.w) <- snps   snps.w[names(i)] <- snps.w[names(i)] + i   snps.w[bim$SNP] <- snps.w[bim$SNP] + w   if(j %% 1000 == 0) cat(sprintf(     '# of processed genes %d: # of annotated %d variants (%.2f%%) \\n',     j, nsnps, nsnps/M*100   ))   return(snps.w) } M <- nrow(bim) snps.w <- Reduce(mapw, 1:nrow(gene.df), init=c()) summary(snps.w) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  # 0.00000 0.02038 0.04425 0.06551 0.07747 1.65767"},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"min-max-normalization","dir":"Articles","previous_headings":"Continuous annotation","what":"Min-max normalization","title":"Genomic annotations","text":"One straightforward normalization method involves min-max normalization, normalized D value signifies degree selected variant pertains annotated category. Run sHDL minmax normalization.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction' D.minmax <- sHDL:::normD(snps.w, LD.path, norm.method='minmax')  # Applied `minmax` weight nomalization on 121235 (39.424%) annotated variants. # The theoretical upper boundary for enrichment fold is 61.419.  summary(D.minmax) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  # 0.00000 0.00000 0.00000 0.01628 0.02030 1.00000 LD.path <- 'UKB_array_SVD_eigen90_extraction' data(gwas1.example, package='HDL') res <- sHDL::sHDL(   snps.w, gwas1.example, LD.path, nthreads=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL, norm.method='minmax' )  str(res) # List of 13 #  $ time         : num 64 #  $ fold         : num 8.74 #  $ h2           : num 0.164 #  $ intercept    : num 0.963 #  $ fold.se      : num 0.91 #  $ h2.se        : num 0.00546 #  $ intercept.se : num 0.0107 #  $ fold.p       : num 1.72e-17 #  $ h2.p         : num 2.69e-198 #  $ intercept.p  : num 0 #  $ stepwise     : logi TRUE #  $ converged    : logi TRUE #  $ optim.message: chr \"CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL\""},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"scaled-normalization","dir":"Articles","previous_headings":"Continuous annotation","what":"Scaled normalization","title":"Genomic annotations","text":"Alternatively, another approach scale sum(D) number annotated variants. Run sHDL scaled normalization.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction' D.scaled <- sHDL:::normD(snps.w, LD.path, norm.method='scaled')  # Applied `scaled` weight nomalization on 121235 (39.424%) annotated variants. # The theoretical upper boundary for enrichment fold is 2.537.  summary(D.scaled) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #  0.0000  0.0000  0.0000  0.3942  0.4916 24.2134 LD.path <- 'UKB_array_SVD_eigen90_extraction' data(gwas1.example, package='HDL') res <- sHDL::sHDL(   snps.w, gwas1.example, LD.path, nthreads=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL, norm.method='scaled' )  str(res) # List of 13 #  $ time         : num 73 #  $ fold         : num 1.2 #  $ h2           : num 0.164 #  $ intercept    : num 0.963 #  $ fold.se      : num 0.0231 #  $ h2.se        : num 0.00546 #  $ intercept.se : num 0.0107 #  $ fold.p       : num 1.73e-17 #  $ h2.p         : num 2.69e-198 #  $ intercept.p  : num 0 #  $ stepwise     : logi TRUE #  $ converged    : logi TRUE #  $ optim.message: chr \"CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH\""},{"path":"https://now2014.github.io/sHDL/articles/Annotation.html","id":"no-normalization","dir":"Articles","previous_headings":"Continuous annotation","what":"No normalization","title":"Genomic annotations","text":"One can also choose keep original annotation weights without normalization. Run sHDL without annotation weights normalization.","code":"LD.path <- 'UKB_array_SVD_eigen90_extraction' D.none <- sHDL:::normD(snps.w, LD.path, norm.method='none')  # No normalization applied on 121235 (39.424%) annotated variants. The theoretical upper boundary for enrichment fold is M / Md = 37.051.  summary(D.none) #    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  # 0.00000 0.00000 0.00000 0.02699 0.03366 1.65767 LD.path <- 'UKB_array_SVD_eigen90_extraction' data(gwas1.example, package='HDL') res <- sHDL::sHDL(   snps.w, gwas1.example, LD.path, nthreads=4,   stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL, norm.method='none' )  str(res) # List of 13 #  $ time         : num 70.8 #  $ fold         : num 5.62 #  $ h2           : num 0.164 #  $ intercept    : num 0.963 #  $ fold.se      : num 0.543 #  $ h2.se        : num 0.00546 #  $ intercept.se : num 0.0107 #  $ fold.p       : num 1.73e-17 #  $ h2.p         : num 2.69e-198 #  $ intercept.p  : num 0 #  $ stepwise     : logi TRUE #  $ converged    : logi TRUE #  $ optim.message: chr \"CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL\""},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"Example of sHDL","text":"install latest version sHDL package via Github, run following code R: data.table package also required example:","code":"# install.packages('remotes') remotes::install_github(\"now2014/sHDL\", ref='main') install.packages('data.table')"},{"path":[]},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"pre-computed-ld-reference-panels","dir":"Articles","previous_headings":"Data preparation","what":"Pre-computed LD reference panels","title":"Example of sHDL","text":"three publically available pre-computed autosome LD reference panels European-ancestry population (335,265 Genomic British individuals UK Biobank) HDL: Panel 1. QCed UK Biobank Axiom Array (307,519 SNPs): Panel 2. QCed UK Biobank imputed HapMap2 (769,306 SNPs): Panel 3. QCed UK Biobank imputed HapMap3 (1,029,876 SNPs): may download one reference panel according SNP density GWAS summary statistics, build LD reference panel. Alternatively, can download reference panels via Baidu Netdisk referring FAQ. , illustrate usage sHDL Panel 2. QCed UK Biobank Axiom Array. Step 01. Download LD reference panel. Step 02. Extract files archive.","code":"wget -c -t 1 \\   https://www.dropbox.com/s/fuvpwsf6r8tjd6c/UKB_array_SVD_eigen90_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_array_SVD_eigen90_extraction.tar.gz wget -c -t 1 \\   https://www.dropbox.com/s/4vuktycxz1an6sp/UKB_imputed_hapmap2_SVD_eigen99_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_imputed_hapmap2_SVD_eigen99_extraction.tar.gz wget -c -t 1 \\   https://www.dropbox.com/s/6js1dzy4tkc3gac/UKB_imputed_SVD_eigen99_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_imputed_SVD_eigen99_extraction.tar.gz wget -c -t 1 https://www.dropbox.com/s/fuvpwsf6r8tjd6c/UKB_array_SVD_eigen90_extraction.tar.gz?dl=0 \\ --no-check-certificate -O /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz # checking the md5 = ff3fadd7ea08bd29759b6c652618cd1f md5sum /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz  # extraction tar -xvf /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"gwas-summary-statistics","dir":"Articles","previous_headings":"Data preparation","what":"GWAS summary statistics","title":"Example of sHDL","text":"can test dataset HDL directly: can prepare data follows: Step 01. Download Height GWAS summary statistics GIANT consortium. Step 02. Load GWAS summary statistics R. Step 03. Select & rename 4 required columns sHDL: SNP: rsid variants A1: effect allele A2: allele Z: GWAS Z-score N: Sample size","code":"data(gwas1.example, package='HDL') # GWAS summary statistics from the Neale Lab round 2 GWAS of UK Biobank of birth weight. gwas.df <- gwas1.example[, c('SNP', 'A1', 'A2', 'Z', 'N')] saveRDS(gwas1.example, file='gwas.df.rds') # save the gwas.df for sHDL wget -c https://portals.broadinstitute.org/collaboration/giant/images/f/f7/GIANT_HEIGHT_YENGO_2022_GWAS_SUMMARY_STATS_EUR.gz gwas.df <- data.table::fread('/path/to/GIANT_HEIGHT_YENGO_2022_GWAS_SUMMARY_STATS_EUR.gz', fill=TRUE, header=TRUE)  head(gwas.df, 5) # show the first five rows #           SNPID       RSID CHR    POS EFFECT_ALLELE OTHER_ALLELE EFFECT_ALLELE_FREQ         BETA      SE        P       N # 1: 1:566875:C:T  rs2185539   1 566875             T            C           0.002800 -0.046315600 0.03930 0.238128  537968 # 2: 1:728951:C:T rs11240767   1 728951             T            C           0.000356  0.167358000 0.12600 0.185025   85591 # 3: 1:734462:A:G rs12564807   1 734462             A            G           0.893000  0.004656900 0.01100 0.672866  112953 # 4: 1:752721:A:G  rs3131972   1 752721             G            A           0.840000  0.000544089 0.00284  0.84811  615932 # 5: 1:754182:A:G  rs3131969   1 754182             G            A           0.865000  0.001333110 0.00185 0.470389 1100634 gwas.df$Z <- gwas.df$BETA / gwas.df$SE # calculate the GWAS Z-score gwas.df <- gwas.df[, c('RSID', 'EFFECT_ALLELE', 'OTHER_ALLELE', 'Z', 'N')] colnames(gwas.df) <- c('SNP', 'A1', 'A2', 'Z', 'N')  head(gwas.df, 5) #           SNP A1 A2          Z       N # 1:  rs2185539  T  C -1.1785140  537968 # 2: rs11240767  T  C  1.3282381   85591 # 3: rs12564807  A  G  0.4233545  112953 # 4:  rs3131972  G  A  0.1915806  615932 # 5:  rs3131969  G  A  0.7206000 1100634  saveRDS(gwas.df, file='gwas.df.rds') # save the gwas.df for sHDL"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"genomic-annotation","dir":"Articles","previous_headings":"Data preparation","what":"Genomic annotation","title":"Example of sHDL","text":"example, create binary annotation genes based gene locations (hg19). Step 01. Download gene location information GeneBreak package. Step 02. Create binary annotation genes. Notice: sHDL automatically align D LD reference panel according rsid, set weights missing variants LD reference panel ZERO.","code":"wget -c https://code.bioconductor.org/browse/GeneBreak/blob/RELEASE_3_18/data/ens.gene.ann.hg19.rda load('ens.gene.ann.hg19.rda') # load ens.gene.ann.hg19 LD.path <- '/your/path/to/UKB_array_SVD_eigen90_extraction'  ## extract gene locations ## gene.pos <- unique(ens.gene.ann.hg19[, c(3:5)]) colnames(gene.pos) <- c('chrom', 'start', 'end') autosomes <- as.character(1:22) gene.pos <- gene.pos[gene.pos$chrom %in% autosomes, ] gene.pos$chrom <- as.numeric(gene.pos$chrom)  ## extract variants position from LD reference panel ## bim <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE )) bim <- bim[, c(2, 1, 4)] colnames(bim) <- c('SNP', 'chrom', 'pos')   ## match annotated variants ## gene2snp <- function(i, j){   cur.chrom <- gene.pos[j, 'chrom']   s <- gene.pos[j, 'start']   e <- gene.pos[j, 'end']   bim <- bim[bim$chrom == cur.chrom, ]   bim <- bim[bim$pos >= s & bim$pos <= e, ]   snps <- unique(c(i, bim$SNP))   nsnps <- length(snps)   if(j %% 1000 == 0) cat(sprintf(     '# of processed genes %d: # of annotated %d variants (%.2f%%) \\n',     j, nsnps, nsnps/M*100   ))   return(snps) } M <- nrow(bim) snps <- Reduce(gene2snp, 1:nrow(gene.pos), init=c())  ## create the annotation weights for sHDL ## D <- rep(1, length(snps)) names(D) <- snps  saveRDS(D, file='D.rds') # save the annotation weights for sHDL"},{"path":"https://now2014.github.io/sHDL/articles/Example.html","id":"run-shdl","dir":"Articles","previous_headings":"","what":"Run sHDL","title":"Example of sHDL","text":"sHDL finish ~3 mins 4 CPU cores. message console analysis (birth weight HDL) :","code":"library(sHDL)  LD.path <- '/your/path/to/UKB_array_SVD_eigen90_extraction' gwas.df <- readRDS('gwas.df.rds') D <- readRDS('D.rds')  t0 <- Sys.time() res <- sHDL::sHDL(D, gwas.df, LD.path, nthreads=4, stepwise=TRUE, lam.cut=1, mode='memory', Dr.path=NULL) print(Sys.time() - t0) Formating GWAS summary statistics ...  307519 out of 307519 (100%) SNPs in reference panel are available in the GWAS.   Converting z (D) to zr (Dr) ...  Applied `minmax` weight nomalization on 147849 (48.078%) annotated variants. The theoretical upper boundary for enrichment fold is 2.080. Optimizing ...  Optimization done in 63.731 seconds.  time    63.7308533191681 fold    1.27398058787877 h2      0.16391651163241 intercept       0.963235890727237 fold.se 0.0259721362117597 h2.se   0.00545604327753226 intercept.se    0.0106589577723641 fold.p  5.13295973147164e-26 h2.p    2.68604947190569e-198 intercept.p     0 stepwise        TRUE converged       TRUE optim.message   CONVERGENCE: NORM OF PROJECTED GRADIENT <= PGTOL Time difference of 2.521161 mins"},{"path":[]},{"path":"https://now2014.github.io/sHDL/articles/MultiGWAS.html","id":"step-01--convert-gwas-summary-statistics-to-mathbfz_r","dir":"Articles","previous_headings":"","what":"Step 01. Convert GWAS summary statistics to \\(\\mathbf{z}_r\\)","title":"Run sHDL across a large number of traits","text":"","code":"#!/usr/bin/env Rscript library(sHDL)  ## function to read GWAS Z-scores and sample size N from GWAS summary statistics ## readZ <- function(gwas.file, LD.path, all.snps){   cat('Reading', gwas.file, '\\n')   gwas.df <- data.table::fread(gwas.file, header=TRUE)   gwas.df <- sHDL:::format.gwas(gwas.df, LD.path)   Z <- rep(0, length(all.snps))   names(Z) <- all.snps   Z[gwas.df$SNP] <- gwas.df$Z   Z <- unname(Z)   N <- median(gwas.df$N, na.rm=TRUE)   return(c(N, Z)) }   ## extract all snps in the LD reference panel ## LD.path <- '/path/to/UKB_array_SVD_eigen90_extraction' all.snps <- do.call(rbind, lapply(   sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), data.table::fread, header=FALSE, select=2 )) all.snps <- unname(unlist(all.snps))  ## combine all GWAS Z-scores into a large matrix matZ ## gwas.dir <- '/path/to/all/gwas/summary/statistics' gwas.files <- list.files(gwas.dir, pattern='*.gz', full.names=TRUE, recursive=FALSE) phes <- gsub('\\\\.gz', '', basename(gwas.files)) # extract phenotype name phe2N <- rep(0, length(phes)) matZ <- matrix(0, ncol=length(phes), nrow=length(all.snps)) colnames(matZ) <- phes rownames(matZ) <- all.snps  for(i in seq_along(gwas.files)){   zz <- readZ(gwas.files[i], LD.path, all.snps)   phe2N[i] <- zz[1]   matZ[, i] <- zz[-1] }  ## Convert matZ to zr ## lam.cut <- 1 nthreads <- 8 ref.data <- sHDL.reduct.dim(   LD.path, z=Z, D=NULL, lam.cut=lam.cut, Dr.path=NULL,   overwrite=FALSE, mode='memory', nthreads=nthreads ) zr <- lapply(ref.data, function(x) x$zr) rs <- unlist(lapply(zr, function(x) nrow(x)))  ## convert zr to list by phes zr <- do.call(rbind, zr) zr <- lapply(1:ncol(zr), function(i, rs=NULL, phes=NULL){     zz <- zr[, i]     e <- cumsum(rs)     s <- c(1, e[-length(e)]+1)     zz <- sapply(1:length(s), function(j) zz[s[j]:e[j]])     return(zz)   }, rs=rs, phes=colnames(Z) ) names(zr) <- colnames(Z) save(zr, phe2N, file='zr-phe2N.rda')"},{"path":"https://now2014.github.io/sHDL/articles/MultiGWAS.html","id":"step-02--convert-genomic-annotations-to-mathbfd_r","dir":"Articles","previous_headings":"","what":"Step 02. Convert genomic annotations to \\(\\mathbf{D}_r\\)","title":"Run sHDL across a large number of traits","text":"","code":"#!/usr/bin/env Rscript  read.annot <- function(annot.name){   ## write this function to return the D vector of given annot.name }  library(sHDL)  overwrite <- FALSE nthreads <- 4 lam.cut <- 1 out.dir <- 'Dr-lam-1' LD.path <- '/path/to/UKB_imputed_SVD_eigen99_extraction' annot.names <- c('all', 'your', 'annot', 'names')  for(annot.name in annot.names){   Dr.path <- paste0(out.dir, '/', annot.name)   D <- read.annot(annot.name)   ref.data <- sHDL.reduct.dim(     LD.path, z=NULL, D=D, lam.cut=lam.cut, Dr.path=Dr.path,     overwrite=FALSE, mode='disk', nthreads=nthreads   ) }"},{"path":"https://now2014.github.io/sHDL/articles/MultiGWAS.html","id":"step-03--run-shdl-in-parallel","dir":"Articles","previous_headings":"","what":"Step 03. Run sHDL in parallel","title":"Run sHDL across a large number of traits","text":"","code":"#!/usr/bin/env Rscript library(sHDL) library(parallel)  run.sHDL <- function(phe, ref.data, phe2N, zr, tsv.dir,   overwrite=FALSE){   tsv.file <- paste0(tsv.dir, '/', phe, '.tsv')   log.file <- paste0(tsv.dir, '/', phe, '.log')   if(file.exists(tsv.file) && !overwrite) return(NULL)   # add zr to ref.data   for(i in seq_along(ref.data)){     ref.data[[i]]$zr <- zr[[phe]][[i]]   }   start.v <- c(1, 0.1, 1)   res <- sHDL::sHDL.optim(ref.data, phe2N[phe], start.v=start.v,     output.file=tsv.file, log.file=log.file, stepwise = TRUE, verbose=TRUE   )   return(res) }  load.Drs <- function(Dr.path){   Drs <- lapply(sHDL:::list.LD.ref.files(Dr.path), function(i){     load(i)     return(list(Dr=Dr, lam=lam, Md=Md, M=M))   })   return(Drs) }  overwrite <- FALSE nthreads <- 10 lam.cut <- 1 out.dir <- 'res-sHDL' Dr.dir <- 'Dr-lam-1'  annot.names <- c('all', 'your', 'annot', 'names') load('zr-phe2N.rda') # zr, phe2N phes <- names(zr)   for(annot.name in annot.names){   Dr.path <- paste0(Dr.dir, '/', annot.name)   tsv.dir <- paste0(out.dir, '/', annot.name)   ref.data <- load.Drs(Dr.path)   if(!dir.exists(tsv.dir)) dir.create(tsv.dir, recursive=TRUE)    cat('annotation:', annot.name, '\\n')     tmp <- mclapply(phes, run.sHDL, zr=zr, phe2N=phe2N, ref.data=ref.data,     tsv.dir=tsv.dir, overwrite=overwrite, mc.cores=nthreads) }"},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"missing-snps-warning","dir":"Articles","previous_headings":"","what":"Missing SNPs warning","title":"Mismatched LD reference panel","text":"matched LD reference panel vital analysis GWAS summary statistics. running sHDL (HDL), may encounter following warning message: default, sHDL (HDL) fills GWAS z-scores zeros SNPs LD reference panel missing.","code":"Warning: More than 1% SNPs in reference panel are missed ... This may generate bias in estimation. Please make sure that you are using correct reference panel."},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"solution","dir":"Articles","previous_headings":"","what":"Solution","title":"Mismatched LD reference panel","text":"address issue, provide function rebuild LD reference panel removing missed SNPs. Caution: GWAS summary statistics severe missingness, heritability may severely underestimated efficiency might lost. rebuilding process resolve mismatch cross-ancestry LD structures.","code":""},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"example","dir":"Articles","previous_headings":"","what":"Example","title":"Mismatched LD reference panel","text":"Suppose gone tutorial prepared LD reference panel, installed sHDL HDL packages. example, demo QCed UK Biobank Axiom Array (307,519 SNPs) reference panel.","code":""},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"simulate-mismatched-gwas-snps","dir":"Articles","previous_headings":"Example","what":"Simulate mismatched GWAS SNPs","title":"Mismatched LD reference panel","text":"simulate mismatched GWAS SNPs, randomly subset 80% SNPs LD reference panel.","code":"library(sHDL) LD.path <- '/your/path/to/UKB_array_SVD_eigen90_extraction' all.snps <- do.call(rbind, lapply(sHDL:::list.LD.ref.files(LD.path, suffix='.bim'), read.table, header=FALSE))[, 2] set.seed(1234) gwas.snps <- sample(all.snps, size=floor(length(all.snps) * 0.8), replace=FALSE)"},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"re-build-the-ld-reference-panel","dir":"Articles","previous_headings":"Example","what":"Re-build the LD reference panel","title":"Mismatched LD reference panel","text":"rebuild LD reference panel gwas.snps store new LD reference panel /path//new/UKB_array_SVD_eigen90_extraction. process may take , can put following code script run background nohup. message console process:","code":"LD.path.new <- '/path/to/new/UKB_array_SVD_eigen90_extraction' sHDL.rebuild.ref(LD.path, gwas.snps, LD.path.new, nthreads = 8) # run with 8 CPU cores Cleaning the new directory... Updating chr1_1 in 21.80 seconds: 6477 (out of 8090) SNPs are kept. Updating chr1_2 in 21.22 seconds: 6468 (out of 8090) SNPs are kept. Updating chr1_3 in 21.69 seconds: 6493 (out of 8090) SNPs are kept. Updating chr2_1 in 21.14 seconds: 6502 (out of 8156) SNPs are kept. Updating chr2_2 in 21.81 seconds: 6558 (out of 8156) SNPs are kept. Updating chr2_3 in 22.30 seconds: 6487 (out of 8154) SNPs are kept. Updating chr3_1 in 15.25 seconds: 5573 (out of 6935) SNPs are kept. Updating chr3_2 in 14.38 seconds: 5502 (out of 6935) SNPs are kept. Updating chr3_3 in 14.37 seconds: 5524 (out of 6935) SNPs are kept. Updating chr4_1 in 37.38 seconds: 7905 (out of 9777) SNPs are kept. Updating chr4_2 in 37.14 seconds: 7870 (out of 9776) SNPs are kept. Updating chr5_1 in 31.47 seconds: 7363 (out of 9260) SNPs are kept. Updating chr5_2 in 31.76 seconds: 7396 (out of 9260) SNPs are kept. Updating chr6_1 in 3.75 seconds: 3054 (out of 3825) SNPs are kept. Updating chr6_2 in 13.50 seconds: 5386 (out of 6757) SNPs are kept. Updating chr6_3 in 14.17 seconds: 5431 (out of 6756) SNPs are kept. Updating chr7_1 in 25.18 seconds: 6931 (out of 8576) SNPs are kept. Updating chr7_2 in 27.01 seconds: 6880 (out of 8575) SNPs are kept. Updating chr8_1 in 20.67 seconds: 6347 (out of 7954) SNPs are kept. Updating chr8_2 in 21.24 seconds: 6389 (out of 7953) SNPs are kept. Updating chr9_1 in 15.26 seconds: 5575 (out of 6949) SNPs are kept. Updating chr9_2 in 14.73 seconds: 5579 (out of 6948) SNPs are kept. Updating chr10_1 in 19.80 seconds: 6200 (out of 7742) SNPs are kept. Updating chr10_2 in 20.10 seconds: 6138 (out of 7741) SNPs are kept. Updating chr11_1 in 17.51 seconds: 5962 (out of 7513) SNPs are kept. Updating chr11_2 in 17.70 seconds: 6038 (out of 7513) SNPs are kept. Updating chr12_1 in 17.84 seconds: 5964 (out of 7470) SNPs are kept. Updating chr12_2 in 17.99 seconds: 5983 (out of 7469) SNPs are kept. Updating chr13_1 in 8.01 seconds: 4386 (out of 5495) SNPs are kept. Updating chr13_2 in 8.21 seconds: 4433 (out of 5495) SNPs are kept. Updating chr14_1 in 7.03 seconds: 4076 (out of 5094) SNPs are kept. Updating chr14_2 in 6.39 seconds: 3979 (out of 5094) SNPs are kept. Updating chr15_1 in 6.50 seconds: 4026 (out of 5023) SNPs are kept. Updating chr15_2 in 6.45 seconds: 4021 (out of 5023) SNPs are kept. Updating chr16_1 in 8.47 seconds: 4494 (out of 5592) SNPs are kept. Updating chr16_2 in 8.46 seconds: 4472 (out of 5591) SNPs are kept. Updating chr17_1 in 6.92 seconds: 4123 (out of 5172) SNPs are kept. Updating chr17_2 in 6.98 seconds: 4115 (out of 5171) SNPs are kept. Updating chr18_1 in 38.63 seconds: 7818 (out of 9783) SNPs are kept. Updating chr19_1 in 26.94 seconds: 6991 (out of 8711) SNPs are kept. Updating chr20_1 in 25.60 seconds: 6797 (out of 8512) SNPs are kept. Updating chr21_1 in 6.81 seconds: 3957 (out of 4988) SNPs are kept. Updating chr22_1 in 8.26 seconds: 4352 (out of 5420) SNPs are kept. [1] \"/path/to/new/UKB_array_SVD_eigen90_extraction\""},{"path":"https://now2014.github.io/sHDL/articles/Rebuild.html","id":"test-the-new-ld-reference-panel","dir":"Articles","previous_headings":"Example","what":"Test the new LD reference panel","title":"Mismatched LD reference panel","text":"Test sHDL::sHDL Test HDL::HDL.h2","code":"data(gwas1.example, package='HDL') M <- nrow(gwas1.example) set.seed(1234) D <- rbinom(M, 1, 0.1) # random D vector names(D) <- gwas1.example$SNP  res.sHDL <- sHDL(   D, gwas1.example, LD.path.new, nthreads=4,   stepwise=TRUE, lam.cut=1, Dr.path=NULL, mode=\"memory\" ) # Formating GWAS summary statistics ...  # 246015 out of 246015 (100%) SNPs in reference panel are available in the GWAS. # Converting z (D) to zr (Dr) ...  # Applied `minmax` weight nomalization on 24634 (10.013)% annotated variants. # The upper boundary for enrichment fold is 9.987. # Optimizing ...  # Optimization done in 27.523 seconds.  # time  27.5229136943817 # fold  1 # h2  0.150313183380116 # intercept 0.985128189915527 # fold.se 0.126907671398178 # h2.se 0.00564781178384186 # intercept.se  0.012627945250898 # fold.p  1 # h2.p  4.62367704900812e-156 # intercept.p 0 # stepwise  TRUE # converged TRUE # optim.message CONVERGENCE: REL_REDUCTION_OF_F <= FACTR*EPSMCH data(gwas1.example, package='HDL') HDL::HDL.h2(gwas1.example, LD.path.new)  # 246015 out of 246015 (100%) SNPs in reference panel are available in the GWAS. # Estimation is ongoing ... 100%  # Integrating piecewise results  # Point estimate: # Heritability:  0.1407  # Continuing computing standard error with jackknife # Progress... 100%  # Heritability:  0.1407 (0.0073) # P:  8.09e-82  # $h2 # [1] 0.1407243  # $h2.se # [1] 0.007344952  # $P # [1] 8.093035e-82  # $eigen.use # [1] 0.9"},{"path":"https://now2014.github.io/sHDL/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Ao Lan. Author, maintainer. Xia Shen. Author.","code":""},{"path":"https://now2014.github.io/sHDL/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lan . Shen X. Modeling genetic architecture complex traits via stratified high-definition likelihood. (2024).","code":"@Article{,   title = {Modeling the genetic architecture of complex traits via stratified high-definition likelihood},   author = {Ao Lan and Xia Shen},   journal = {.},   year = {2024},   volume = {0},   number = {0},   pages = {0-0}, }"},{"path":[]},{"path":"https://now2014.github.io/sHDL/index.html","id":"introduction","dir":"","previous_headings":"","what":"Introduction","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"practice, density GWAS SNPs often poses challenge ensuring positive definiteness covariance matrix Σ GWAS Z-score vector z. presents significant obstacle conducting statistical modeling, positive definite Σ essential. Moreover, high density SNPs results high-dimensional problem. stratified high-definition likelihood (sHDL) extends full-likelihood framework high-definition likelihood (HDL) method (Ning et al. 2020), addressing issue transforming Σ reduced, positive definite covariance matrix Σr. Paired Cholesky decomposition, sHDL method proves highly efficient heritability enrichment analysis.","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"install latest version sHDL package via Github, run following code R:","code":"# install.packages('remotes') remotes::install_github(\"now2014/sHDL\", ref='main')"},{"path":"https://now2014.github.io/sHDL/index.html","id":"quick-vignette","dir":"","previous_headings":"","what":"Quick vignette","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"Download LD reference panel HDL software: Run sHDL","code":"wget -c -t 1 \\   https://www.dropbox.com/s/fuvpwsf6r8tjd6c/UKB_array_SVD_eigen90_extraction.tar.gz?dl=0 \\   --no-check-certificate -O /path/to/UKB_array_SVD_eigen90_extraction.tar.gz  # checking the md5 = ff3fadd7ea08bd29759b6c652618cd1f md5sum /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz  # extraction tar -xvf /your/path/to/UKB_array_SVD_eigen90_extraction.tar.gz remotes::install_github(\"zhenin/HDL/HDL\") data(gwas1.example, package=\"HDL\")  library(sHDL)  ## The GWAS summary statistics for birth weight loaded from HDL package. M <- nrow(gwas1.example) set.seed(1234) D <- rbinom(M, 1, 0.01) # random D vector names(D) <- gwas1.example$SNP  ## The path to the directory where linkage disequilibrium (LD) information is stored. LD.path <- \"/your/path/to/UKB_array_SVD_eigen90_extraction\"  ## To speed up the test, we set a large lam.cut value. res.sHDL <- sHDL(D, gwas1.example, LD.path, nthreads=4, stepwise=TRUE, lam.cut=10, Dr.path=NULL, mode=\"memory\") print(as.data.frame(res.sHDL))"},{"path":"https://now2014.github.io/sHDL/index.html","id":"more-tutorials","dir":"","previous_headings":"","what":"More tutorials","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"detail please see tutorials https://now2014.github.io/sHDL/.","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"use sHDL software, please cite: Lan . Shen X. Modeling genetic architecture complex traits via stratified high-definition likelihood. (2024). Ning, Z., Pawitan, Y. & Shen, X. High-definition likelihood inference genetic correlations across human complex traits. Nat Genet (2020).","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"contact","dir":"","previous_headings":"","what":"Contact","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"Reporting bugs opening new issue Github page. Sending email authors: Ao Lan Xia Shen.","code":""},{"path":"https://now2014.github.io/sHDL/index.html","id":"future-plan","dir":"","previous_headings":"","what":"Future plan","title":"Stratified high-definition likelihood inference of heritability\n        enrichment","text":"repository integrated HDL software future.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":null,"dir":"Reference","previous_headings":"","what":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Perform heritability enrichment analysis trait based GWAS summary statistics.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"","code":"sHDL(   D,   gwas.df,   LD.path,   output.file = NULL,   nthreads = 1,   stepwise = TRUE,   fill.missing.N = c(\"none\", \"min\", \"max\", \"median\", \"mean\"),   lim = exp(-18),   lam.cut = NULL,   Dr.path = \"./Dr\",   overwrite = FALSE,   verbose = FALSE,   fix.h2 = NULL,   fix.intercept = NULL,   maxit = 1000,   pgtol = 0.001,   start.v = c(1, 0.1, 1),   lwr = NULL,   upr = NULL,   mode = c(\"disk\", \"memory\"),   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\",   norm.method = c(\"minmax\", \"scaled\", \"none\") )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"D vector genomic annotations vector names SNP IDs. gwas.df data frame including GWAS summary statistics genetic variants trait. input data frame include following columns: SNP, SNP ID; A1, effect allele; A2, reference allele; N, sample size; Z, z-score; Z given, alternatively, may provide: b, estimate marginal effect GWAS; se, standard error estimates marginal effects GWAS. LD.path Path directory linkage disequilibrium (LD) information stored, compatible HDL LD reference panels. output.file log results written. specify file, log printed console. nthreads Number threads use, default nthreads = 1. stepwise Whether estimate enrichment fold estimating heritability intercept first, default stepwise = FALSE. fix.h2 fix.intercept NULL, stepwise overridden. fill.missing.N sample size missing GWAS summary statistics, fill.missing.N used fill missing values. lim Tolerance limitation ensure positive-definiteness covariance matrices, default lim = exp(-18). lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff. Dr.path Path directory Dr matrices stored, default Dr.path = \"./Dr\". overwrite Whether overwrite existing Dr matrices, default overwrite = FALSE. verbose Whether print log console, default verbose = FALSE. fix.h2 Whether fix heritability fix.h2 estimate heritability, default fix.h2 = NULL, means estimate heritability. fix.intercept Whether fix intercept fix.intercept estimate intercept, default fix.intercept = NULL, means estimate intercept. maxit Maximum number iterations, default maxit = 1000. pgtol Tolerance convergence, default pgtol = 1e-3. start.v vector starting values c(fold, h2, intercept) optimization. lwr Lower bounds c(fold, h2, intercept). Default NULL, means c(0, 0, 0.1). upr Upper bounds c(fold, h2, intercept). Default NULL, means c(M / Md, 1, 5), Md sum annotation weights M total number SNPs. mode Whether store Dr disk memory, default mode = \"disk\". mode = \"disk\", Dr stored disk (path returned ) lam returned. mode = \"memory\", Dr lam returned. pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\". norm.method normalization method, either \"minmax\" (default), \"scaled\" \"none\". \"minmax\", annotation weight vector D normalized [0, 1]. \"scaled\", sum normalized vector D scaled number annotated SNPs. \"none\", annotation weight vector D normalized.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"list returned : time time used optimization. fold estimated heritability enrichment fold. h2 estimated SNP-based heritability. intercept estimated intercept. fold.se standard error estimated heritability enrichment fold. h2.se standard error estimated heritability. intercept.se standard error estimated intercept. fold.p P-value based Wald test estimated heritability enrichment fold. h2.p P-value based Wald test estimated heritability. intercept.p P-value based Wald test estimated intercept. stepwise Whether optimization done stepwise manner. converged Whether optimization converges. message message returned optim.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"note","dir":"Reference","previous_headings":"","what":"Note","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Users can download precomputed eigenvalues eigenvectors LD correlation matrices European ancestry population. download link can found https://github.com/zhenin/HDL/wiki/Reference-panels LD matrices eigen-decomposition 335,265 genomic British UK Biobank individuals. Three sets reference panel provided: 1) 1,029,876 QCed UK Biobank imputed HapMap3 SNPs. size 33 GB unzipping. Although takes time, using imputed panel provides accurate estimates genetic correlations. Therefore GWAS includes HapMap3 SNPs, recommend using imputed reference panel. 2) 769,306 QCed UK Biobank imputed HapMap2 SNPs. size 18 GB unzipping.one GWAS includes HapMap 2 SNPs, many SNPs (1 HapMap2 panel proper used HDL. 3) 307,519 QCed UK Biobank Axiom Array SNPs. size 7.5 GB unzipping.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Lan Shen X (2024). Modeling Genetic Architecture Complex Traits via Stratified High-Definition Likelihood.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"Ao Lan, Xia Shen","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Perform heritability enrichment analysis of trait based on GWAS summary statistics. — sHDL","text":"","code":"if (FALSE) { ## The GWAS summary statistics for birth weight loaded from HDL package. data(gwas1.example, package='HDL') M <- nrow(gwas1.example) set.seed(1234) D <- rbinom(M, 1, 0.01) # random D vector names(D) <- gwas1.example$SNP  ## The path to the directory where linkage disequilibrium (LD) information is stored. LD.path <- \"path/to/UKB_array_SVD_eigen90_extraction\"  ## To speed up the test, we set a large lam.cut value. res.sHDL <- sHDL(D, gwas1.example, LD.path, nthreads=4, stepwise=TRUE, lam.cut=10, Dr.path=NULL, mode=\"memory\") print(as.data.frame(res.sHDL)) }"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":null,"dir":"Reference","previous_headings":"","what":"Maximize the log likelihood to infer the heritability enrichment fold. — sHDL.optim","title":"Maximize the log likelihood to infer the heritability enrichment fold. — sHDL.optim","text":"Maximize log likelihood infer heritability enrichment fold.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Maximize the log likelihood to infer the heritability enrichment fold. — sHDL.optim","text":"","code":"sHDL.optim(   ref.data,   N,   start.v = c(1, 0.1, 1),   output.file = NULL,   log.file = \"\",   stepwise = FALSE,   fix.h2 = NULL,   fix.intercept = NULL,   lim = exp(-18),   verbose = FALSE,   clust = NULL,   lwr = NULL,   upr = NULL,   maxit = 1000,   pgtol = 0.001 )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Maximize the log likelihood to infer the heritability enrichment fold. — sHDL.optim","text":"ref.data list data matched LD reference generated sHDL.reduct.dim. N sample size GWAS. start.v vector starting values c(fold, h2, intercept) optimization. output.file log results written. log.file log written. stepwise Whether estimate enrichment fold estimating heritability intercept first, default stepwise = FALSE. fix.h2 fix.intercept NULL, stepwise overridden. fix.h2 Whether fix heritability fix.h2 estimate heritability, default fix.h2 = NULL, means estimate heritability. fix.intercept Whether fix intercept fix.intercept estimate intercept, default fix.intercept = NULL, means estimate intercept. lim Tolerance limitation ensure positive-definiteness covariance matrices, default lim = exp(-18). verbose Whether print log console, default verbose = FALSE. clust cluster object generated parallel::makeCluster. lwr Lower bounds c(fold, h2, intercept). Default NULL, means c(0, 0, 0.1). upr Upper bounds c(fold, h2, intercept). Default NULL, means c(M / Md, 1, 5), Md sum annotation weights M total number SNPs. maxit Maximum number iterations, default maxit = 1000. pgtol Tolerance convergence, default pgtol = 1e-3.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.optim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Maximize the log likelihood to infer the heritability enrichment fold. — sHDL.optim","text":"list returned : time Time elapsed seconds optimization. fold estimated heritability enrichment fold. h2 estimated SNP-based heritability. intercept estimated intercept. fold.se standard error estimated heritability enrichment fold. h2.se standard error estimated heritability. intercept.se standard error estimated intercept. fold.p P-value based Wald test estimated heritability enrichment fold. h2.p P-value based Wald test estimated heritability. intercept.p P-value based Wald test estimated intercept. stepwise Whether optimization done stepwise manner. converged Whether optimization converges. message message returned optim.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":null,"dir":"Reference","previous_headings":"","what":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"Re-build LD reference panel given SNPs.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"","code":"sHDL.rebuild.ref(   LD.path,   gwas.snps,   LD.path.new,   nthreads = 1,   lam.cut = NULL,   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\" )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"LD.path Path .rda file Eigen decomposition LD matrix stored. gwas.snps vector contains SNP IDs GWAS summary statistics. LD.path.new new path re-built LD reference. nthreads Number threads use, default nthreads = 1. lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff original LD reference panel. pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\".","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.rebuild.ref.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Re-build the LD reference panel with given SNPs. — sHDL.rebuild.ref","text":"new path re-built LD reference.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"Reduce dimension covariance matrix converting z (D) zr (Dr).","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"","code":"sHDL.reduct.dim(   LD.path,   z = NULL,   D = NULL,   lam.cut = NULL,   Dr.path = NULL,   overwrite = FALSE,   mode = c(\"disk\", \"memory\"),   nthreads = 1,   pattern = \".*chr(\\\\d{1,2})\\\\.(\\\\d{1,2})[_\\\\.].*\",   norm.method = c(\"minmax\", \"scaled\", \"none\") )"},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"LD.path Path .rda file Eigen decomposition LD matrix stored. z matrix Z-scores rownames SNP IDs. Supporting multiple columns multiple traits. D vector genomic annotations vector names SNP IDs. lam.cut Eigenvalue cutoff LD matrices, default lam.cut = NULL, means cutoff. Dr.path Path directory Dr matrices stored, default Dr.path = NULL, means store Dr disk. overwrite Whether overwrite existing Dr matrices, default overwrite = FALSE. mode Whether store Dr disk memory, default mode = \"disk\". mode = \"disk\", Dr stored disk (path returned ) lam returned. mode = \"memory\", Dr lam returned. nthreads Number threads use matrix operations, default nthreads = 1. pattern Chromosome picece pattern LD files, default \".*chr(\\d{1,2})\\.(\\d{1,2})[_\\.].*\". norm.method normalization method, either \"minmax\" (default), \"scaled\" \"none\". \"minmax\", annotation weight vector D normalized [0, 1]. \"scaled\", sum normalized vector D scaled number annotated SNPs. \"none\", annotation weight vector D normalized.","code":""},{"path":"https://now2014.github.io/sHDL/reference/sHDL.reduct.dim.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce the dimension of covariance matrix by converting z (D) to zr (Dr). — sHDL.reduct.dim","text":"list returned : Dr reduct RDR matrix. zr reduct z-score vector. lam eigenvalues LD matrix. Md sum annotation weights SNPs given LD matrix. M number SNPs given LD matrix.","code":""}]
